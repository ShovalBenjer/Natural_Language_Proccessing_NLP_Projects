{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxeRrj4VscbMIGmuigLGi6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShovalBenjer/Natural_Language_Proccessing_NLP_Projects/blob/main/MultiOneHotEncoder_Amazon_Reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e_Fby-9eOnh0",
        "outputId": "de866217-6309-4e91-b6b6-6cb258a5923d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0 market_place customer_id         review_id    product_id  \\\n",
            "0           0         \"US\"  \"25933450\"   \"RJOVP071AVAJO\"  \"0439873800\"   \n",
            "1           1         \"US\"   \"1801372\"  \"R1ORGBETCDW3AI\"  \"1623953553\"   \n",
            "2           2         \"US\"   \"5782091\"   \"R7TNRFQAOUTX5\"  \"142151981X\"   \n",
            "3           3         \"US\"  \"32715830\"  \"R2GANXKDIFZ6OI\"  \"014241543X\"   \n",
            "4           4         \"US\"  \"14005703\"  \"R2NYB6C3R8LVN6\"  \"1604600527\"   \n",
            "\n",
            "  product_parent                                   product_title  \\\n",
            "0     \"84656342\"  \"There Was an Old Lady Who Swallowed a Shell!\"   \n",
            "1    \"729938122\"                                \"I Saw a Friend\"   \n",
            "2    \"678139048\"                          \"Black Lagoon, Vol. 6\"   \n",
            "3    \"712432151\"                                     \"If I Stay\"   \n",
            "4    \"800572372\"                       \"Stars 'N Strips Forever\"   \n",
            "\n",
            "  product_category  star_rating  helpful_votes  total_votes     vine  \\\n",
            "0          \"Books\"            1              0            0  0 \\t(N)   \n",
            "1          \"Books\"            1              0            0  0 \\t(N)   \n",
            "2          \"Books\"            1              0            0  0 \\t(N)   \n",
            "3          \"Books\"            1              0            0  0 \\t(N)   \n",
            "4          \"Books\"            1              2            2  0 \\t(N)   \n",
            "\n",
            "  verified_purchase                                    review_headline  \\\n",
            "0           1 \\t(Y)                                       \"Five Stars\"   \n",
            "1           1 \\t(Y)  \"Please buy \"I Saw a Friend\"! Your children wi...   \n",
            "2           1 \\t(Y)                                    \"Shipped fast.\"   \n",
            "3           0 \\t(N)                                       \"Five Stars\"   \n",
            "4           1 \\t(Y)                                       \"Five Stars\"   \n",
            "\n",
            "                                         review_body review_date  \\\n",
            "0               \"I love it and so does my students!\"  2015-08-31   \n",
            "1  \"My wife and I ordered 2 books and gave them a...  2015-08-31   \n",
            "2  \"Great book just like all the others in the se...  2015-08-31   \n",
            "3                                     \"So beautiful\"  2015-08-31   \n",
            "4  \"Enjoyed the author's story and his quilts are...  2015-08-31   \n",
            "\n",
            "  Sentiment_books review_month review_day  review_year  \n",
            "0        positive       August     Monday         2015  \n",
            "1        positive       August     Monday         2015  \n",
            "2        positive       August     Monday         2015  \n",
            "3        positive       August     Monday         2015  \n",
            "4        positive       August     Monday         2015  \n",
            "Index(['Unnamed: 0', 'market_place', 'customer_id', 'review_id', 'product_id',\n",
            "       'product_parent', 'product_title', 'product_category', 'star_rating',\n",
            "       'helpful_votes', 'total_votes', 'vine', 'verified_purchase',\n",
            "       'review_headline', 'review_body', 'review_date', 'Sentiment_books',\n",
            "       'review_month', 'review_day', 'review_year'],\n",
            "      dtype='object')\n",
            "Unnamed: 0           0\n",
            "market_place         0\n",
            "customer_id          0\n",
            "review_id            0\n",
            "product_id           0\n",
            "product_parent       0\n",
            "product_title        0\n",
            "product_category     0\n",
            "star_rating          0\n",
            "helpful_votes        0\n",
            "total_votes          0\n",
            "vine                 0\n",
            "verified_purchase    0\n",
            "review_headline      0\n",
            "review_body          0\n",
            "review_date          0\n",
            "Sentiment_books      0\n",
            "review_month         0\n",
            "review_day           0\n",
            "review_year          0\n",
            "dtype: int64\n",
            "\n",
            "Distribution of 'overall' ratings BEFORE sentiment conversion:\n",
            "overall\n",
            "1    84\n",
            "0    16\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Distribution of 'sentiment' AFTER sentiment conversion:\n",
            "sentiment\n",
            "0    100\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Class distribution BEFORE train_test_split:\n",
            "sentiment\n",
            "0    100\n",
            "Name: count, dtype: int64\n",
            "Tokenization of the first 3 reviews:\n",
            "Review 1: ['\"', 'i', 'love', 'it', 'and', 'so', 'does', 'my', 'students', '!', '\"']\n",
            "Review 2: ['\"', 'my', 'wife', 'and', 'i', 'ordered', '2', 'books', 'and', 'gave', 'them', 'as', 'presents', '.', '.', '.', 'one', 'to', 'a', \"friend's\", 'daughter', 'and', 'the', 'other', 'to', 'our', 'grandson', '!', 'both', 'children', 'were', 'so', 'happy', 'with', 'the', 'story', ',', 'by', 'author', 'katrina', 'streza', ',', 'and', 'they', 'were', 'overjoyed', 'with', 'the', 'absolutely', 'adorable', 'artwork', ',', 'by', 'artist', 'michele', 'katz', ',', 'throughout', 'the', 'book', '!', 'we', 'highly', 'recommend', '34', ';', 'i', 'saw', 'a', 'friend', '34', ';', 'to', 'all', 'your', 'little', 'ones', '!', '!', '!', '\"']\n",
            "Review 3: ['\"', 'great', 'book', 'just', 'like', 'all', 'the', 'others', 'in', 'the', 'series', '.', '\"']\n",
            "\n",
            "Lexicon based sentiment analysis evaluation:\n",
            "  After 10 examples, accuracy: 30.00%\n",
            "  After 20 examples, accuracy: 20.00%\n",
            "  After 30 examples, accuracy: 26.67%\n",
            "  After 40 examples, accuracy: 35.00%\n",
            "Final accuracy over 50 examples: 34.00%\n",
            "\n",
            "Finding minimal examples for >70% accuracy (Lexicon method):\n",
            "  Accuracy did not exceed 70% within the first 100 examples.\n",
            "\n",
            "Vocabulary size: 114\n",
            "\n",
            "Class distribution BEFORE train_test_split:\n",
            "0    100\n",
            "Name: count, dtype: int64\n",
            "ERROR: Only one class found in the data. Cannot perform classification.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package opinion_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'## Conclusion\\nThis concludes the notebook. The last cell shows the accuracy of the logistic Regression model on the test set, this model is more accurate than the lexicon method used.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Amazon_Reviews_Sentiment_Analysis.ipynb (Corrected Again - Single Class Error Debug) - Corrected Version\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "import math\n",
        "import nltk\n",
        "from nltk.corpus import opinion_lexicon\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# Download required NLTK resources (if not already downloaded)\n",
        "nltk.download('opinion_lexicon')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'amazon_books_Data.csv'\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, nrows=1000, quoting=csv.QUOTE_ALL, escapechar='\\\\')\n",
        "    except UnicodeDecodeError:\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, nrows=100, encoding='latin-1', quoting=csv.QUOTE_ALL, escapechar='\\\\')\n",
        "        except UnicodeDecodeError:\n",
        "            df = pd.read_csv(file_path, nrows=100, encoding='cp1252', quoting=csv.QUOTE_ALL, escapechar='\\\\')\n",
        "else:\n",
        "    raise FileNotFoundError(f\"The file {file_path} does not exist. Please make sure it's in the current directory.\")\n",
        "\n",
        "print(df.head())\n",
        "print(df.columns)\n",
        "print(df.isnull().sum())\n",
        "\n",
        "df.dropna(subset=['review_body', 'star_rating'], inplace=True)\n",
        "df['overall'] = df['star_rating']\n",
        "\n",
        "def safe_int_conversion(value):\n",
        "    try:\n",
        "        return int(float(value))\n",
        "    except (ValueError, TypeError):\n",
        "        return -1\n",
        "\n",
        "df['overall'] = df['overall'].apply(safe_int_conversion)\n",
        "df = df[df['overall'] != -1]\n",
        "\n",
        "print(\"\\nDistribution of 'overall' ratings BEFORE sentiment conversion:\")\n",
        "print(df['overall'].value_counts())\n",
        "\n",
        "df['sentiment'] = df['overall'].apply(lambda x: 1 if x >= 4 else 0)\n",
        "\n",
        "print(\"\\nDistribution of 'sentiment' AFTER sentiment conversion:\")\n",
        "print(df['sentiment'].value_counts())\n",
        "print(\"\\nClass distribution BEFORE train_test_split:\")\n",
        "print(df['sentiment'].value_counts())\n",
        "\n",
        "\"\"\"## Part 1: Regular Expressions for Tokenization\"\"\"\n",
        "\n",
        "# Pre-compile the regex pattern for efficiency (outside the function for better re-use if tokenize is called many times)\n",
        "TOKEN_PATTERN = re.compile(r\"[\\w]+(?:[-'\\w]{0,3}[\\w]+)?|[.!?,\\\";]\")\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Tokenizes the input text based on the specified rules using a pre-compiled regex.\n",
        "\n",
        "    Args:\n",
        "        text: The input text string.\n",
        "\n",
        "    Returns:\n",
        "        A list of tokens.\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or text is None:\n",
        "        return []\n",
        "    text = str(text).lower()\n",
        "    tokens = TOKEN_PATTERN.findall(text)\n",
        "    return tokens\n",
        "\n",
        "print(\"Tokenization of the first 3 reviews:\")\n",
        "for i in range(min(3, len(df))):\n",
        "    review = df['review_body'].iloc[i]\n",
        "    tokens = tokenize(review)\n",
        "    print(f\"Review {i+1}: {tokens}\")\n",
        "\n",
        "\"\"\"## Part 2: Applying a Lexicon for Sentiment Classification\"\"\"\n",
        "\n",
        "def lexicaScore(lexicon_pos, lexicon_neg, tokens):\n",
        "    \"\"\"\n",
        "    Calculates the relative frequency of positive and negative words in the tokens.\n",
        "\n",
        "    Args:\n",
        "        lexicon_pos: A set of positive words.\n",
        "        lexicon_neg: A set of negative words.\n",
        "        tokens: A list of tokens.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary with 'pos' and 'neg' keys representing relative frequencies.\n",
        "    \"\"\"\n",
        "    pos_count = 0\n",
        "    neg_count = 0\n",
        "    total_count = len(tokens)\n",
        "\n",
        "    if total_count == 0:\n",
        "        return {'pos': 0.0, 'neg': 0.0}\n",
        "\n",
        "    for token in tokens:\n",
        "        if token in lexicon_pos:\n",
        "            pos_count += 1\n",
        "        elif token in lexicon_neg:\n",
        "            neg_count += 1\n",
        "\n",
        "    return {'pos': pos_count / total_count, 'neg': neg_count / total_count}\n",
        "\n",
        "positive_lexicon = set(opinion_lexicon.positive())\n",
        "negative_lexicon = set(opinion_lexicon.negative())\n",
        "\n",
        "def predict_sentiment(lexicon_pos, lexicon_neg, tokens, thresh_pos=0.05, thresh_neg=0.05):\n",
        "    \"\"\"\n",
        "    Predicts sentiment based on lexicon scores and thresholds.\n",
        "    Returns 1 for positive, 0 for negative, or defaults to majority class for neutral cases.\n",
        "    \"\"\"\n",
        "    scores = lexicaScore(lexicon_pos, lexicon_neg, tokens)\n",
        "    if scores['pos'] > thresh_pos and scores['pos'] > scores['neg']:\n",
        "        return 1  # Positive\n",
        "    elif scores['neg'] > thresh_neg and scores['neg'] > scores['pos']:\n",
        "        return 0  # Negative\n",
        "    else:\n",
        "        if df['sentiment'].mean() > 0.5:  # Default to majority class (positive if majority)\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "print(\"\\nLexicon based sentiment analysis evaluation:\")\n",
        "correct_predictions = 0\n",
        "num_examples = 0\n",
        "\n",
        "for i in range(len(df)):\n",
        "    review = df['review_body'].iloc[i]\n",
        "    true_sentiment = df['sentiment'].iloc[i]\n",
        "    tokens = tokenize(review)\n",
        "    predicted_sentiment = predict_sentiment(positive_lexicon, negative_lexicon, tokens)\n",
        "\n",
        "    if predicted_sentiment == true_sentiment:\n",
        "        correct_predictions += 1\n",
        "    num_examples += 1\n",
        "\n",
        "    if num_examples >= 50:\n",
        "        break\n",
        "\n",
        "    if (num_examples%10) == 0:\n",
        "      accuracy = (correct_predictions / num_examples) * 100\n",
        "      print(f\"  After {num_examples} examples, accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "print(f\"Final accuracy over {num_examples} examples: {(correct_predictions / num_examples) * 100:.2f}%\")\n",
        "\n",
        "\n",
        "print(\"\\nFinding minimal examples for >70% accuracy (Lexicon method):\")\n",
        "correct_predictions = 0\n",
        "num_examples = 0\n",
        "for i in range(len(df)):\n",
        "    review = df['review_body'].iloc[i]\n",
        "    true_sentiment = df['sentiment'].iloc[i]\n",
        "    tokens = tokenize(review)\n",
        "    predicted_sentiment = predict_sentiment(positive_lexicon, negative_lexicon, tokens, thresh_pos=0.02, thresh_neg=0.04) # Experiment with thresholds\n",
        "\n",
        "    if predicted_sentiment == true_sentiment:\n",
        "        correct_predictions += 1\n",
        "    num_examples += 1\n",
        "\n",
        "    accuracy = (correct_predictions / num_examples) * 100\n",
        "    if accuracy > 70:\n",
        "        print(f\"  Accuracy exceeded 70% after {num_examples} examples.\")\n",
        "        break\n",
        "else:\n",
        "    print(f\"  Accuracy did not exceed 70% within the first {num_examples} examples.\")\n",
        "\n",
        "\"\"\"## Part 3: Logistic Regression for Sentiment Classification\"\"\"\n",
        "\n",
        "def extractMultiHot(tokens, vocab):\n",
        "    \"\"\"\n",
        "    Creates a multi-hot encoding of the tokens based on the vocabulary.\n",
        "\n",
        "    Args:\n",
        "        tokens: A list of tokens.\n",
        "        vocab: A dictionary mapping words to their indices in the vocabulary.\n",
        "\n",
        "    Returns:\n",
        "        A list representing the multi-hot encoding.\n",
        "    \"\"\"\n",
        "    vocab_size = len(vocab)\n",
        "    multi_hot = [0] * vocab_size\n",
        "    for token in tokens:\n",
        "        if token in vocab:\n",
        "            index = vocab.get(token)\n",
        "            if index is not None and 0 <= index < vocab_size:\n",
        "                multi_hot[index] = 1\n",
        "    return multi_hot\n",
        "\n",
        "def negative_log_likelihood(ypred, ytrue):\n",
        "    \"\"\"\n",
        "    Calculates the negative log likelihood for a single instance.\n",
        "    This is NOT normalized by the number of observations in a typical sense.\n",
        "\n",
        "    Args:\n",
        "        ypred: Predicted probability (output of logistic regression) for the positive class (1).\n",
        "        ytrue: True label (0 or 1).\n",
        "\n",
        "    Returns:\n",
        "        The negative log likelihood for this instance. Returns None if input is invalid.\n",
        "    \"\"\"\n",
        "    if not (0 <= ypred <= 1 and (ytrue == 0 or ytrue == 1)):\n",
        "        return None\n",
        "    epsilon = 1e-15 # For numerical stability\n",
        "    ypred = max(epsilon, min(1 - epsilon, ypred)) # Clip predicted probability to avoid log(0)\n",
        "\n",
        "    log_likelihood = ytrue * math.log(ypred) + (1 - ytrue) * math.log(1 - ypred)\n",
        "    return -log_likelihood\n",
        "\n",
        "\n",
        "word_counts = Counter()\n",
        "for review in df['review_body']:\n",
        "    tokens = tokenize(review)\n",
        "    word_counts.update(tokens)\n",
        "\n",
        "vocabulary = {word: i for i, (word, count) in enumerate(word_counts.items()) if count > 5}\n",
        "print(f\"\\nVocabulary size: {len(vocabulary)}\")\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "    review = df['review_body'].iloc[i]\n",
        "    tokens = tokenize(review)\n",
        "    multi_hot = extractMultiHot(tokens, vocabulary)\n",
        "    X.append(multi_hot)\n",
        "    y.append(df['sentiment'].iloc[i])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "print(\"\\nClass distribution BEFORE train_test_split:\")\n",
        "print(pd.Series(y).value_counts())\n",
        "\n",
        "if len(np.unique(y)) < 2:\n",
        "    print(\"ERROR: Only one class found in the data. Cannot perform classification.\")\n",
        "else:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "    print(\"\\nClass distribution in training set:\")\n",
        "    print(pd.Series(y_train).value_counts())\n",
        "\n",
        "    model = LogisticRegression(solver='liblinear', random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"\\nLogistic Regression Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    total_log_loss = 0\n",
        "    num_valid_losses = 0\n",
        "    for i in range(len(y_test)):\n",
        "        log_loss_val = negative_log_likelihood(y_pred_proba[i], y_test[i]) # Using negative_log_likelihood function\n",
        "        if log_loss_val is not None:\n",
        "            total_log_loss += log_loss_val\n",
        "            num_valid_losses += 1\n",
        "\n",
        "    if num_valid_losses > 0:\n",
        "        average_log_loss = total_log_loss / num_valid_losses\n",
        "        print(f\"Average Negative Log Loss: {average_log_loss:.4f}\") # Corrected name in output\n",
        "    else:\n",
        "        print(\"No valid log loss values were calculated.\")\n",
        "\n",
        "\"\"\"## Conclusion\n",
        "This concludes the notebook. The last cell shows the accuracy of the logistic Regression model on the test set, this model is more accurate than the lexicon method used.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Amazon_Reviews_Sentiment_Analysis.ipynb (Corrected Again - Single Class Error Debug V2)\"\"\"\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "import math\n",
        "import nltk\n",
        "from nltk.corpus import opinion_lexicon\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import csv\n",
        "import os\n",
        "\n",
        "nltk.download('opinion_lexicon')\n",
        "nltk.download('punkt')\n",
        "\n",
        "file_path = 'amazon_books_Data.csv'\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, nrows=1000, quoting=csv.QUOTE_ALL, escapechar='\\\\')\n",
        "    except UnicodeDecodeError:\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, nrows=1000, encoding='latin-1', quoting=csv.QUOTE_ALL, escapechar='\\\\')\n",
        "        except UnicodeDecodeError:\n",
        "            df = pd.read_csv(file_path, nrows=1000, encoding='cp1252', quoting=csv.QUOTE_ALL, escapechar='\\\\')\n",
        "else:\n",
        "    raise FileNotFoundError(f\"File not found: {file_path}\")\n",
        "\n",
        "print(df.head())\n",
        "print(df.columns)\n",
        "print(df.isnull().sum())\n",
        "\n",
        "df.dropna(subset=['review_body', 'star_rating'], inplace=True)\n",
        "df['overall'] = df['star_rating']\n",
        "\n",
        "def safe_int_conversion(value):\n",
        "    try:\n",
        "        return int(float(value))\n",
        "    except (ValueError, TypeError):\n",
        "        return -1\n",
        "\n",
        "df['overall'] = df['overall'].apply(safe_int_conversion)\n",
        "df = df[df['overall'] != -1]\n",
        "\n",
        "# *** NEW DEBUGGING OUTPUT: Distribution of original star_rating ***\n",
        "print(\"\\nDistribution of original 'star_rating' column:\")\n",
        "print(df['star_rating'].value_counts())\n",
        "\n",
        "# *** DEBUGGING OUTPUT: Distribution of 'overall' ratings BEFORE sentiment conversion ***\n",
        "print(\"\\nDistribution of 'overall' ratings BEFORE sentiment conversion (after safe_int):\")\n",
        "print(df['overall'].value_counts())\n",
        "\n",
        "# Sentiment conversion\n",
        "df['sentiment'] = df['overall'].apply(lambda x: 1 if x >= 4 else 0)\n",
        "\n",
        "# *** DEBUGGING OUTPUT: Distribution of 'sentiment' AFTER sentiment conversion ***\n",
        "print(\"\\nDistribution of 'sentiment' AFTER sentiment conversion:\")\n",
        "print(df['sentiment'].value_counts())\n",
        "\n",
        "# Check class distribution before split\n",
        "print(\"\\nClass distribution BEFORE train_test_split:\")\n",
        "print(df['sentiment'].value_counts())\n",
        "\n",
        "\n",
        "# Part 1, 2, 3 code (rest of your notebook) remains the same from the last corrected version.\n",
        "# ... (rest of your code from the last corrected version) ..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btQAJXziYAWS",
        "outputId": "b03770dd-4599-441f-c217-d245a09e516d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0 market_place customer_id         review_id    product_id  \\\n",
            "0           0         \"US\"  \"25933450\"   \"RJOVP071AVAJO\"  \"0439873800\"   \n",
            "1           1         \"US\"   \"1801372\"  \"R1ORGBETCDW3AI\"  \"1623953553\"   \n",
            "2           2         \"US\"   \"5782091\"   \"R7TNRFQAOUTX5\"  \"142151981X\"   \n",
            "3           3         \"US\"  \"32715830\"  \"R2GANXKDIFZ6OI\"  \"014241543X\"   \n",
            "4           4         \"US\"  \"14005703\"  \"R2NYB6C3R8LVN6\"  \"1604600527\"   \n",
            "\n",
            "  product_parent                                   product_title  \\\n",
            "0     \"84656342\"  \"There Was an Old Lady Who Swallowed a Shell!\"   \n",
            "1    \"729938122\"                                \"I Saw a Friend\"   \n",
            "2    \"678139048\"                          \"Black Lagoon, Vol. 6\"   \n",
            "3    \"712432151\"                                     \"If I Stay\"   \n",
            "4    \"800572372\"                       \"Stars 'N Strips Forever\"   \n",
            "\n",
            "  product_category  star_rating  helpful_votes  total_votes     vine  \\\n",
            "0          \"Books\"            1              0            0  0 \\t(N)   \n",
            "1          \"Books\"            1              0            0  0 \\t(N)   \n",
            "2          \"Books\"            1              0            0  0 \\t(N)   \n",
            "3          \"Books\"            1              0            0  0 \\t(N)   \n",
            "4          \"Books\"            1              2            2  0 \\t(N)   \n",
            "\n",
            "  verified_purchase                                    review_headline  \\\n",
            "0           1 \\t(Y)                                       \"Five Stars\"   \n",
            "1           1 \\t(Y)  \"Please buy \"I Saw a Friend\"! Your children wi...   \n",
            "2           1 \\t(Y)                                    \"Shipped fast.\"   \n",
            "3           0 \\t(N)                                       \"Five Stars\"   \n",
            "4           1 \\t(Y)                                       \"Five Stars\"   \n",
            "\n",
            "                                         review_body review_date  \\\n",
            "0               \"I love it and so does my students!\"  2015-08-31   \n",
            "1  \"My wife and I ordered 2 books and gave them a...  2015-08-31   \n",
            "2  \"Great book just like all the others in the se...  2015-08-31   \n",
            "3                                     \"So beautiful\"  2015-08-31   \n",
            "4  \"Enjoyed the author's story and his quilts are...  2015-08-31   \n",
            "\n",
            "  Sentiment_books review_month review_day  review_year  \n",
            "0        positive       August     Monday         2015  \n",
            "1        positive       August     Monday         2015  \n",
            "2        positive       August     Monday         2015  \n",
            "3        positive       August     Monday         2015  \n",
            "4        positive       August     Monday         2015  \n",
            "Index(['Unnamed: 0', 'market_place', 'customer_id', 'review_id', 'product_id',\n",
            "       'product_parent', 'product_title', 'product_category', 'star_rating',\n",
            "       'helpful_votes', 'total_votes', 'vine', 'verified_purchase',\n",
            "       'review_headline', 'review_body', 'review_date', 'Sentiment_books',\n",
            "       'review_month', 'review_day', 'review_year'],\n",
            "      dtype='object')\n",
            "Unnamed: 0           0\n",
            "market_place         0\n",
            "customer_id          0\n",
            "review_id            0\n",
            "product_id           0\n",
            "product_parent       0\n",
            "product_title        0\n",
            "product_category     0\n",
            "star_rating          0\n",
            "helpful_votes        0\n",
            "total_votes          0\n",
            "vine                 0\n",
            "verified_purchase    0\n",
            "review_headline      0\n",
            "review_body          0\n",
            "review_date          0\n",
            "Sentiment_books      0\n",
            "review_month         0\n",
            "review_day           0\n",
            "review_year          0\n",
            "dtype: int64\n",
            "\n",
            "Distribution of original 'star_rating' column:\n",
            "star_rating\n",
            "1    84\n",
            "0    16\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Distribution of 'overall' ratings BEFORE sentiment conversion (after safe_int):\n",
            "overall\n",
            "1    84\n",
            "0    16\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Distribution of 'sentiment' AFTER sentiment conversion:\n",
            "sentiment\n",
            "0    100\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Class distribution BEFORE train_test_split:\n",
            "sentiment\n",
            "0    100\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package opinion_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}