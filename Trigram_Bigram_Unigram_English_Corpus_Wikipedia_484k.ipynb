{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShovalBenjer/Natural_Language_Proccessing_NLP_Projects/blob/main/Trigram_Bigram_Unigram_English_Corpus_Wikipedia_484k.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIYdn1woOS1n",
        "outputId": "e7bababf-185e-441e-a846-51ce7b6ff5f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting N-gram Model Building ---\n",
            "Processing 112 lines...\n",
            "Found 3281 non-empty sentences.\n",
            "Total tokens (including boundary markers): 75779\n",
            "Sample tokens: ['<s>', '<s>', 'the', 'government', 'last', 'july', 'called', 'the', 'energy', 'sector', 'debt', 'situation', 'a', 'state', 'of', 'emergency', '</s>', '<s>', '<s>', 'this', 'was', 'during', 'the', 'midyear', 'budget'] ... ['your', 'hands', 'with', 'soap', 'and', 'water', 'cover', 'your', 'mouth', 'when', 'you', 'cough', 'or', 'sneeze', 'stay', 'home', 'from', 'work', 'until', 'you', 've', 'gone', 'one', 'day', '</s>']\n",
            "\n",
            "--- Counting N-grams ---\n",
            "Total Unigrams (tokens): 75779\n",
            "Unique Unigrams: 9692\n",
            "Total Trigrams: 75777\n",
            "Unique Trigrams: 60325\n",
            "\n",
            "--- Sample N-gram Counts ---\n",
            "Top 5 Unigrams: [(('<s>',), 6562), (('the',), 4050), (('</s>',), 3281), (('to',), 1877), (('of',), 1775)]\n",
            "Top 5 Bigrams: [(('<s>', '<s>'), 3281), (('</s>', '<s>'), 3280), (('of', 'the'), 442), (('<s>', 'the'), 418), (('in', 'the'), 341)]\n",
            "Top 5 Trigrams: [(('</s>', '<s>', '<s>'), 3280), (('<s>', '<s>', 'the'), 418), (('<s>', '<s>', 'it'), 102), (('said', '</s>', '<s>'), 89), (('<s>', '<s>', 'he'), 81)]\n",
            "\n",
            "--- Calculating Probabilities (MLE) ---\n",
            "\n",
            "--- Sample N-gram Probabilities (MLE) ---\n",
            "Unigram Probabilities:\n",
            "  P(<s>) = 0.086594\n",
            "  P(the) = 0.053445\n",
            "  P(</s>) = 0.043297\n",
            "  P(to) = 0.024769\n",
            "  P(of) = 0.023423\n",
            "\n",
            "Bigram Probabilities:\n",
            "  P(<s> | <s>) = 0.500000  (Count(('<s>', '<s>'))=3281, Count(<s>)=6562)\n",
            "  P(<s> | </s>) = 0.999695  (Count(('</s>', '<s>'))=3280, Count(</s>)=3281)\n",
            "  P(the | of) = 0.249014  (Count(('of', 'the'))=442, Count(of)=1775)\n",
            "  P(the | <s>) = 0.063700  (Count(('<s>', 'the'))=418, Count(<s>)=6562)\n",
            "  P(the | in) = 0.241844  (Count(('in', 'the'))=341, Count(in)=1410)\n",
            "\n",
            "Trigram Probabilities:\n",
            "  P(<s> | </s>, <s>) = 1.000000  (Count(('</s>', '<s>', '<s>'))=3280, Count(('</s>', '<s>'))=3280)\n",
            "  P(the | <s>, <s>) = 0.127400  (Count(('<s>', '<s>', 'the'))=418, Count(('<s>', '<s>'))=3281)\n",
            "  P(it | <s>, <s>) = 0.031088  (Count(('<s>', '<s>', 'it'))=102, Count(('<s>', '<s>'))=3281)\n",
            "  P(<s> | said, </s>) = 1.000000  (Count(('said', '</s>', '<s>'))=89, Count(('said', '</s>'))=89)\n",
            "  P(he | <s>, <s>) = 0.024688  (Count(('<s>', '<s>', 'he'))=81, Count(('<s>', '<s>'))=3281)\n",
            "\n",
            "--- Model Building Complete ---\n",
            "\n",
            "\n",
            "--- Generating Sentences ---\n",
            "\n",
            "--- 7 Sentences generated by Unigram Model ---\n",
            "1: the with of minute rated who sanna global important information fight and the home this berry with will released recognised\n",
            "2: protection he time while the it prime england reported more 2020 likewise than absence been maclean health confirm to a\n",
            "3: an of travel to told month s will head cases to the at these will finishes who president after at\n",
            "4: nerf of in use until in policy including removing jan transparent in also be strong dozen officers the three friday\n",
            "5: may the themselves use wolf cases claims phenylketonuria and announced of cost pharmacists a week not a crisis good for\n",
            "6: the public the had and lives of of actually it regulars director a of for they chief have including quarantined\n",
            "7: the children water in most to in questions there said governs throughout transfer our with international one officers united four\n",
            "\n",
            "--- 7 Sentences generated by Bigram Model ---\n",
            "1: <s> in the hotel after france have n't pick up extensively in those infected across the world still be published\n",
            "2: all things get those have left the 120 crawley s 34yearold prime minister\n",
            "3: <s> dana leigh marks it funny when inhaled and safety shield 360 will set to the outbreak 17\n",
            "4: <s> earlier i m\n",
            "5: <s> <s> <s> over funding\n",
            "6: <s> <s> that capitalism\n",
            "7: some kind that the city of soil and he further beyond wuhan s general tony maw commander at the destabilizing\n",
            "\n",
            "--- 7 Sentences generated by Trigram Model ---\n",
            "1: the top court judgment said\n",
            "2: wuhan is history too wuchang uprising the rebellion in the past 30 years\n",
            "3: for the second case in singapore\n",
            "4: and for other purposes\n",
            "5: under the u k\n",
            "6: nortje who strikes me as a longstanding practice the government works to reduce flights to wuhan airport and once they\n",
            "7: there s a link and here s the coldest day of work at a little bit of turn now maybe\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "N-gram Language Model Builder and Generator for Colab\n",
        "\n",
        "Reads a text file, builds unigram, bigram, and trigram models,\n",
        "calculates MLE probabilities, and generates new sentences based on the models.\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import collections\n",
        "from collections import Counter, defaultdict\n",
        "import math\n",
        "import random # Needed for weighted random choice\n",
        "\n",
        "# --- Configuration ---\n",
        "file_path = '/content/20-01.txt'\n",
        "START_TOKEN = \"<s>\"\n",
        "END_TOKEN = \"</s>\"\n",
        "UNKNOWN_TOKEN = \"<UNK>\"\n",
        "\n",
        "# --- 1. Text Preprocessing ---\n",
        "# (Keep the preprocess_text function from the previous version)\n",
        "def preprocess_text(filepath):\n",
        "    \"\"\"Reads a file, converts to lowercase, tokenizes, and adds sentence boundary markers.\"\"\"\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            lines = f.readlines()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {filepath}\")\n",
        "        print(\"Please make sure you have uploaded the file to the /content/ directory.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file: {e}\")\n",
        "        return None\n",
        "\n",
        "    all_tokens = []\n",
        "    print(f\"Processing {len(lines)} lines...\")\n",
        "    processed_sentences = 0\n",
        "    for i, line in enumerate(lines):\n",
        "        try:\n",
        "            line = re.sub(r'^@@\\d+\\s*<[ph]>\\s*', '', line)\n",
        "            line = re.sub(r'<[ph]>', ' ', line)\n",
        "            line = re.sub(r'@@@@@@@@@@\\s*', '', line)\n",
        "            line = re.sub(r'https?\\s*:\\s*\\*\\*.*?\\.\\.\\.', '', line)\n",
        "            line = re.sub(r'\\*\\*.*?\\.\\.\\.', '', line)\n",
        "            line = line.lower()\n",
        "            line = re.sub(r'[^\\w\\s\\'\\.\\?\\!]', '', line)\n",
        "            sentences = re.split(r'(?<=[.?!])(?:\\s+|$)', line.strip())\n",
        "            for sentence in sentences:\n",
        "                sentence = sentence.strip()\n",
        "                if not sentence: continue\n",
        "                words = re.findall(r'\\b\\w+\\'?\\w*\\b', sentence)\n",
        "                if not words: continue\n",
        "                processed_sentences += 1\n",
        "                tokens = [START_TOKEN, START_TOKEN] + words + [END_TOKEN]\n",
        "                all_tokens.extend(tokens)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing line {i+1}: {line[:100]}... Error: {e}\")\n",
        "            continue\n",
        "        # if (i + 1) % 5000 == 0: print(f\"Processed {i+1}/{len(lines)} lines...\") # Reduce frequency if needed\n",
        "\n",
        "    print(f\"Found {processed_sentences} non-empty sentences.\")\n",
        "    print(f\"Total tokens (including boundary markers): {len(all_tokens)}\")\n",
        "    if not all_tokens:\n",
        "        print(\"Warning: No tokens were extracted.\")\n",
        "        return None\n",
        "    if len(all_tokens) < 50: print(\"Sample tokens:\", all_tokens)\n",
        "    else: print(\"Sample tokens:\", all_tokens[:25], \"...\", all_tokens[-25:])\n",
        "    return all_tokens\n",
        "\n",
        "\n",
        "# --- 2. N-gram Counting ---\n",
        "# (Keep the build_ngram_counts function from the previous version)\n",
        "def build_ngram_counts(tokens, n):\n",
        "    \"\"\"Builds counts for n-grams from a list of tokens.\"\"\"\n",
        "    if not tokens: return Counter()\n",
        "    ngrams = []\n",
        "    if len(tokens) >= n:\n",
        "        for i in range(len(tokens) - n + 1):\n",
        "            ngram = tuple(tokens[i : i + n])\n",
        "            ngrams.append(ngram)\n",
        "    else: print(f\"Warning: Not enough tokens ({len(tokens)}) to build any {n}-grams.\")\n",
        "    count_result = Counter(ngrams)\n",
        "    if not count_result and len(tokens) >= n: print(f\"Warning: No {n}-grams were generated despite sufficient tokens.\")\n",
        "    return count_result\n",
        "\n",
        "# --- 3. Probability Calculation (Maximum Likelihood Estimation - MLE) ---\n",
        "# (Keep the calculate_ngram_probabilities function from the previous version)\n",
        "def calculate_ngram_probabilities(ngram_counts, n_minus_1_gram_counts, total_tokens_or_vocab_size, n):\n",
        "    \"\"\"Calculates MLE probabilities for n-grams.\"\"\"\n",
        "    probabilities = defaultdict(float)\n",
        "    if not ngram_counts: return probabilities\n",
        "    if n == 1:\n",
        "        total_count = total_tokens_or_vocab_size\n",
        "        if total_count == 0: return probabilities\n",
        "        for word_tuple, count in ngram_counts.items():\n",
        "            probabilities[word_tuple] = count / total_count\n",
        "    elif n > 1:\n",
        "        if not n_minus_1_gram_counts: return probabilities # Need prefixes\n",
        "        zero_prefix_count = 0\n",
        "        for ngram, count in ngram_counts.items():\n",
        "            prefix = ngram[:-1]\n",
        "            prefix_key = prefix[0] if n == 2 else prefix\n",
        "            prefix_count = n_minus_1_gram_counts.get(prefix_key, 0)\n",
        "            if prefix_count > 0:\n",
        "                probabilities[ngram] = count / prefix_count\n",
        "            else:\n",
        "                probabilities[ngram] = 0.0\n",
        "                zero_prefix_count += 1\n",
        "        # if zero_prefix_count > 0: print(f\"  Note: {zero_prefix_count} ngrams (n={n}) had zero-count prefixes.\")\n",
        "    else: print(f\"Warning: Invalid n ({n}) in calculate_ngram_probabilities.\")\n",
        "    return probabilities\n",
        "\n",
        "# --- 4. Sentence Generation Functions ---\n",
        "\n",
        "def generate_unigram(unigram_probs, max_len=25):\n",
        "    \"\"\"Generates a sentence using the unigram model.\"\"\"\n",
        "    if not unigram_probs:\n",
        "        return \"(Error: Unigram probabilities unavailable)\"\n",
        "\n",
        "    # Get words and their probabilities, excluding boundary tokens for generation start\n",
        "    # We will handle END_TOKEN explicitly during generation\n",
        "    possible_words = [word[0] for word in unigram_probs.keys() if word[0] not in (START_TOKEN, END_TOKEN)]\n",
        "    probabilities = [unigram_probs[ (word,) ] for word in possible_words]\n",
        "\n",
        "    if not possible_words: # Handle case where only boundary tokens exist\n",
        "         return \"(Error: No valid words found in unigram model for generation)\"\n",
        "\n",
        "    # Normalize probabilities (important if they don't sum perfectly to 1 after filtering)\n",
        "    prob_sum = sum(probabilities)\n",
        "    if prob_sum <= 0:\n",
        "         return \"(Error: Cannot generate unigram, invalid probability sum)\"\n",
        "    probabilities = [p / prob_sum for p in probabilities]\n",
        "\n",
        "\n",
        "    generated_sentence = []\n",
        "    for _ in range(max_len):\n",
        "        # Choose a word based on its overall probability\n",
        "        chosen_word = random.choices(possible_words, weights=probabilities, k=1)[0]\n",
        "\n",
        "        # Append the chosen word\n",
        "        generated_sentence.append(chosen_word)\n",
        "\n",
        "    # Join the words. Unigram doesn't naturally produce END_TOKEN based on context.\n",
        "    return \" \".join(generated_sentence)\n",
        "\n",
        "def generate_bigram(bigram_probs, max_len=25):\n",
        "    \"\"\"Generates a sentence using the bigram model.\"\"\"\n",
        "    if not bigram_probs:\n",
        "        return \"(Error: Bigram probabilities unavailable)\"\n",
        "\n",
        "    current_word = START_TOKEN\n",
        "    generated_sentence = []\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        # Find possible next words based on the current word\n",
        "        possible_next_options = {\n",
        "            bg[1]: prob for bg, prob in bigram_probs.items() if bg[0] == current_word\n",
        "        }\n",
        "\n",
        "        if not possible_next_options:\n",
        "            # Dead end - no known word follows the current one\n",
        "            break\n",
        "\n",
        "        next_words = list(possible_next_options.keys())\n",
        "        probabilities = list(possible_next_options.values())\n",
        "\n",
        "        # Normalize probabilities to ensure they sum to 1 for random.choices\n",
        "        prob_sum = sum(probabilities)\n",
        "        if prob_sum <= 0:\n",
        "            break # Avoid division by zero or errors if probabilities are invalid\n",
        "        probabilities = [p / prob_sum for p in probabilities]\n",
        "\n",
        "        # Choose the next word based on conditional probabilities P(next | current)\n",
        "        try:\n",
        "             chosen_word = random.choices(next_words, weights=probabilities, k=1)[0]\n",
        "        except ValueError:\n",
        "             # Can happen if probabilities list is empty or contains invalid values\n",
        "             print(f\"  Warning (Bi): Value error during random choice. Context: {current_word}, Options: {next_words[:5]}, Probs: {probabilities[:5]}\")\n",
        "             break\n",
        "\n",
        "\n",
        "        if chosen_word == END_TOKEN:\n",
        "            break # Reached the end of the sentence\n",
        "\n",
        "        generated_sentence.append(chosen_word)\n",
        "        current_word = chosen_word # Update context\n",
        "\n",
        "    return \" \".join(generated_sentence) if generated_sentence else \"(Empty bigram sentence)\"\n",
        "\n",
        "\n",
        "def generate_trigram(trigram_probs, max_len=25):\n",
        "    \"\"\"Generates a sentence using the trigram model.\"\"\"\n",
        "    if not trigram_probs:\n",
        "        return \"(Error: Trigram probabilities unavailable)\"\n",
        "\n",
        "    # Initial context is two start tokens\n",
        "    context = (START_TOKEN, START_TOKEN)\n",
        "    generated_sentence = []\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        # Find possible next words based on the current context (last two words)\n",
        "        possible_next_options = {\n",
        "            tg[2]: prob for tg, prob in trigram_probs.items() if (tg[0], tg[1]) == context\n",
        "        }\n",
        "\n",
        "        if not possible_next_options:\n",
        "            # Dead end - no known word follows the current context\n",
        "            # Optionally, could implement backoff to bigram here\n",
        "            break\n",
        "\n",
        "        next_words = list(possible_next_options.keys())\n",
        "        probabilities = list(possible_next_options.values())\n",
        "\n",
        "        # Normalize probabilities\n",
        "        prob_sum = sum(probabilities)\n",
        "        if prob_sum <= 0:\n",
        "            break\n",
        "        probabilities = [p / prob_sum for p in probabilities]\n",
        "\n",
        "        # Choose the next word based on conditional probabilities P(next | context)\n",
        "        try:\n",
        "            chosen_word = random.choices(next_words, weights=probabilities, k=1)[0]\n",
        "        except ValueError:\n",
        "            print(f\"  Warning (Tri): Value error during random choice. Context: {context}, Options: {next_words[:5]}, Probs: {probabilities[:5]}\")\n",
        "            break\n",
        "\n",
        "\n",
        "        if chosen_word == END_TOKEN:\n",
        "            break # Reached the end of the sentence\n",
        "\n",
        "        generated_sentence.append(chosen_word)\n",
        "        # Update context: slide the window\n",
        "        context = (context[1], chosen_word)\n",
        "\n",
        "    return \" \".join(generated_sentence) if generated_sentence else \"(Empty trigram sentence)\"\n",
        "\n",
        "\n",
        "# --- Main Execution ---\n",
        "\n",
        "print(\"--- Starting N-gram Model Building ---\")\n",
        "\n",
        "# 1. Preprocess Text\n",
        "tokens = preprocess_text(file_path)\n",
        "\n",
        "if tokens:\n",
        "    # 2. Count N-grams\n",
        "    print(\"\\n--- Counting N-grams ---\")\n",
        "    unigram_counts = build_ngram_counts(tokens, 1)\n",
        "    bigram_counts = build_ngram_counts(tokens, 2)\n",
        "    trigram_counts = build_ngram_counts(tokens, 3)\n",
        "\n",
        "    if not unigram_counts or not bigram_counts or not trigram_counts:\n",
        "        print(\"\\nError: Failed to generate n-gram counts. Cannot proceed.\")\n",
        "    else:\n",
        "        print(f\"Total Unigrams (tokens): {sum(unigram_counts.values())}\")\n",
        "        print(f\"Unique Unigrams: {len(unigram_counts)}\")\n",
        "        # ... (rest of count printing) ...\n",
        "        print(f\"Total Trigrams: {sum(trigram_counts.values())}\")\n",
        "        print(f\"Unique Trigrams: {len(trigram_counts)}\")\n",
        "\n",
        "        print(\"\\n--- Sample N-gram Counts ---\")\n",
        "        if unigram_counts: print(\"Top 5 Unigrams:\", unigram_counts.most_common(5))\n",
        "        if bigram_counts: print(\"Top 5 Bigrams:\", bigram_counts.most_common(5))\n",
        "        if trigram_counts: print(\"Top 5 Trigrams:\", trigram_counts.most_common(5))\n",
        "\n",
        "        # 3. Calculate Probabilities (MLE)\n",
        "        print(\"\\n--- Calculating Probabilities (MLE) ---\")\n",
        "        total_token_count_for_unigrams = sum(unigram_counts.values())\n",
        "        unigram_prefix_counts_for_bigrams = Counter(token for token in tokens) # Needed for bigram calculation context\n",
        "        # Bigram counts are needed for trigram calculation context\n",
        "\n",
        "        unigram_probs = calculate_ngram_probabilities(unigram_counts, None, total_token_count_for_unigrams, n=1)\n",
        "        bigram_probs = calculate_ngram_probabilities(bigram_counts, unigram_prefix_counts_for_bigrams, None, n=2)\n",
        "        trigram_probs = calculate_ngram_probabilities(trigram_counts, bigram_counts, None, n=3) # Use bigram_counts as context\n",
        "\n",
        "        if not unigram_probs or not bigram_probs or not trigram_probs:\n",
        "             print(\"\\nError: Failed to calculate probabilities. Cannot proceed.\")\n",
        "        else:\n",
        "            print(\"\\n--- Sample N-gram Probabilities (MLE) ---\")\n",
        "            # (Keep the sample probability printing)\n",
        "            # Unigrams\n",
        "            print(\"Unigram Probabilities:\")\n",
        "            if unigram_counts:\n",
        "              sample_unigrams = [ug for ug, count in unigram_counts.most_common(5)]\n",
        "              for ug in sample_unigrams:\n",
        "                  print(f\"  P({ug[0]}) = {unigram_probs.get(ug, 0.0):.6f}\")\n",
        "            else: print(\"  No unigrams to calculate probabilities for.\")\n",
        "\n",
        "            # Bigrams\n",
        "            print(\"\\nBigram Probabilities:\")\n",
        "            if bigram_counts:\n",
        "              sample_bigrams = [bg for bg, count in bigram_counts.most_common(5)]\n",
        "              for bg in sample_bigrams:\n",
        "                  prefix_uni = bg[0] # Prefix is a single token\n",
        "                  print(f\"  P({bg[1]} | {prefix_uni}) = {bigram_probs.get(bg, 0.0):.6f}  (Count({bg})={bigram_counts.get(bg,0)}, Count({prefix_uni})={unigram_prefix_counts_for_bigrams.get(prefix_uni, 0)})\")\n",
        "            else: print(\"  No bigrams to calculate probabilities for.\")\n",
        "\n",
        "\n",
        "            # Trigrams\n",
        "            print(\"\\nTrigram Probabilities:\")\n",
        "            if trigram_counts:\n",
        "              sample_trigrams = [tg for tg, count in trigram_counts.most_common(5)]\n",
        "              for tg in sample_trigrams:\n",
        "                  prefix_bi = tg[:-1] # Prefix is a bigram tuple\n",
        "                  print(f\"  P({tg[2]} | {tg[0]}, {tg[1]}) = {trigram_probs.get(tg, 0.0):.6f}  (Count({tg})={trigram_counts.get(tg,0)}, Count({prefix_bi})={bigram_counts.get(prefix_bi, 0)})\")\n",
        "            else: print(\"  No trigrams to calculate probabilities for.\")\n",
        "\n",
        "\n",
        "            print(\"\\n--- Model Building Complete ---\")\n",
        "\n",
        "            # ***** NEW: SENTENCE GENERATION SECTION *****\n",
        "            print(\"\\n\\n--- Generating Sentences ---\")\n",
        "            num_sentences_to_generate = 7\n",
        "            max_sentence_length = 20 # Limit length to avoid overly long/run-on sentences\n",
        "\n",
        "            print(f\"\\n--- {num_sentences_to_generate} Sentences generated by Unigram Model ---\")\n",
        "            for i in range(num_sentences_to_generate):\n",
        "                generated = generate_unigram(unigram_probs, max_len=max_sentence_length)\n",
        "                print(f\"{i+1}: {generated}\")\n",
        "\n",
        "            print(f\"\\n--- {num_sentences_to_generate} Sentences generated by Bigram Model ---\")\n",
        "            for i in range(num_sentences_to_generate):\n",
        "                generated = generate_bigram(bigram_probs, max_len=max_sentence_length)\n",
        "                print(f\"{i+1}: {generated}\")\n",
        "\n",
        "            print(f\"\\n--- {num_sentences_to_generate} Sentences generated by Trigram Model ---\")\n",
        "            for i in range(num_sentences_to_generate):\n",
        "                generated = generate_trigram(trigram_probs, max_len=max_sentence_length)\n",
        "                print(f\"{i+1}: {generated}\")\n",
        "\n",
        "            # Optional: Keep the user input loop for probability checking if desired\n",
        "            # print(\"\\n--- Calculate Probability for User Input ---\")\n",
        "            # while True:\n",
        "            #     # ... (input loop code from previous answer) ...\n",
        "\n",
        "else:\n",
        "    print(\"\\nCould not process the file or build models. Generation skipped.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}