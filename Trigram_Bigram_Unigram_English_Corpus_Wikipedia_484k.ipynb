{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShovalBenjer/Natural_Language_Proccessing_NLP_Projects/blob/main/Trigram_Bigram_Unigram_English_Corpus_Wikipedia_484k.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIYdn1woOS1n",
        "outputId": "a6c13fff-e19a-4392-9034-69d7b7354b0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting N-gram Model Building ---\n",
            "Processing 112 lines...\n",
            "Found 3281 non-empty sentences.\n",
            "Total tokens (including boundary markers): 75779\n",
            "Sample tokens: ['<s>', '<s>', 'the', 'government', 'last', 'july', 'called', 'the', 'energy', 'sector', 'debt', 'situation', 'a', 'state', 'of', 'emergency', '</s>', '<s>', '<s>', 'this', 'was', 'during', 'the', 'midyear', 'budget'] ... ['your', 'hands', 'with', 'soap', 'and', 'water', 'cover', 'your', 'mouth', 'when', 'you', 'cough', 'or', 'sneeze', 'stay', 'home', 'from', 'work', 'until', 'you', 've', 'gone', 'one', 'day', '</s>']\n",
            "\n",
            "--- Counting N-grams ---\n",
            "Total Unigrams (tokens): 75779\n",
            "Unique Unigrams: 9692\n",
            "Total Bigrams: 75778\n",
            "Unique Bigrams: 42652\n",
            "Total Trigrams: 75777\n",
            "Unique Trigrams: 60325\n",
            "\n",
            "--- Sample N-gram Counts ---\n",
            "Top 5 Unigrams: [(('<s>',), 6562), (('the',), 4050), (('</s>',), 3281), (('to',), 1877), (('of',), 1775)]\n",
            "Top 5 Bigrams: [(('<s>', '<s>'), 3281), (('</s>', '<s>'), 3280), (('of', 'the'), 442), (('<s>', 'the'), 418), (('in', 'the'), 341)]\n",
            "Top 5 Trigrams: [(('</s>', '<s>', '<s>'), 3280), (('<s>', '<s>', 'the'), 418), (('<s>', '<s>', 'it'), 102), (('said', '</s>', '<s>'), 89), (('<s>', '<s>', 'he'), 81)]\n",
            "\n",
            "--- Calculating Probabilities (MLE) ---\n",
            "\n",
            "--- Sample N-gram Probabilities (MLE) ---\n",
            "Unigram Probabilities:\n",
            "  P(<s>) = 0.086594\n",
            "  P(the) = 0.053445\n",
            "  P(</s>) = 0.043297\n",
            "  P(to) = 0.024769\n",
            "  P(of) = 0.023423\n",
            "\n",
            "Bigram Probabilities:\n",
            "  P(<s> | <s>) = 0.500000  (Count(('<s>', '<s>'))=3281, Count(<s>)=6562)\n",
            "  P(<s> | </s>) = 0.999695  (Count(('</s>', '<s>'))=3280, Count(</s>)=3281)\n",
            "  P(the | of) = 0.249014  (Count(('of', 'the'))=442, Count(of)=1775)\n",
            "  P(the | <s>) = 0.063700  (Count(('<s>', 'the'))=418, Count(<s>)=6562)\n",
            "  P(the | in) = 0.241844  (Count(('in', 'the'))=341, Count(in)=1410)\n",
            "\n",
            "Trigram Probabilities:\n",
            "  P(<s> | </s>, <s>) = 1.000000  (Count(('</s>', '<s>', '<s>'))=3280, Count(('</s>', '<s>'))=3280)\n",
            "  P(the | <s>, <s>) = 0.127400  (Count(('<s>', '<s>', 'the'))=418, Count(('<s>', '<s>'))=3281)\n",
            "  P(it | <s>, <s>) = 0.031088  (Count(('<s>', '<s>', 'it'))=102, Count(('<s>', '<s>'))=3281)\n",
            "  P(<s> | said, </s>) = 1.000000  (Count(('said', '</s>', '<s>'))=89, Count(('said', '</s>'))=89)\n",
            "  P(he | <s>, <s>) = 0.024688  (Count(('<s>', '<s>', 'he'))=81, Count(('<s>', '<s>'))=3281)\n",
            "\n",
            "--- Model Building Complete ---\n",
            "\n",
            "--- Calculating Probabilities for Example Sentences ---\n",
            "\n",
            "--- Probabilities for sentence: 'the government last july called the energy sector' ---\n",
            "  Model      | Log Probability | Raw Probability (Approx.)\n",
            "-------------|-----------------|---------------------------\n",
            "  Unigram    |        -55.7789 | 5.963742e-25\n",
            "  Bigram     |        -30.7634 | 4.361272e-14\n",
            "  Trigram    |        -20.2798 | 1.558028e-09\n",
            "  (Using basic Add-1 smoothing for unseen n-grams)\n",
            "  (Note: Raw probabilities are often extremely small; log probabilities are standard for comparison)\n",
            "\n",
            "--- Probabilities for sentence: 'this is a completely random sentence' ---\n",
            "  Model      | Log Probability | Raw Probability (Approx.)\n",
            "-------------|-----------------|---------------------------\n",
            "  Unigram    |        -47.1663 | 3.280388e-21\n",
            "  Bigram     |        -44.4097 | 5.165434e-20\n",
            "  Trigram    |        -43.3251 | 1.528098e-19\n",
            "  (Using basic Add-1 smoothing for unseen n-grams)\n",
            "  (Note: Raw probabilities are often extremely small; log probabilities are standard for comparison)\n",
            "\n",
            "--- Calculate Probability for User Input ---\n",
            "Enter a sentence to calculate its probability (or press Enter to quit): are you fine\n",
            "\n",
            "--- Probabilities for sentence: 'are you fine' ---\n",
            "  Model      | Log Probability | Raw Probability (Approx.)\n",
            "-------------|-----------------|---------------------------\n",
            "  Unigram    |        -20.2305 | 1.636777e-09\n",
            "  Bigram     |        -33.0566 | 4.402620e-15\n",
            "  Trigram    |        -35.6333 | 3.347063e-16\n",
            "  (Using basic Add-1 smoothing for unseen n-grams)\n",
            "  (Note: Raw probabilities are often extremely small; log probabilities are standard for comparison)\n",
            "Enter a sentence to calculate its probability (or press Enter to quit): you\n",
            "\n",
            "--- Probabilities for sentence: 'you' ---\n",
            "  Model      | Log Probability | Raw Probability (Approx.)\n",
            "-------------|-----------------|---------------------------\n",
            "  Unigram    |         -5.7221 | 3.272674e-03\n",
            "  Bigram     |         -8.7891 | 1.523926e-04\n",
            "  Trigram    |        -13.8442 | 9.717514e-07\n",
            "  (Using basic Add-1 smoothing for unseen n-grams)\n",
            "  (Note: Raw probabilities are often extremely small; log probabilities are standard for comparison)\n",
            "Enter a sentence to calculate its probability (or press Enter to quit): This was during\n",
            "\n",
            "--- Probabilities for sentence: 'This was during' ---\n",
            "  Model      | Log Probability | Raw Probability (Approx.)\n",
            "-------------|-----------------|---------------------------\n",
            "  Unigram    |        -18.5525 | 8.765265e-09\n",
            "  Bigram     |        -24.2187 | 3.033427e-11\n",
            "  Trigram    |        -17.6805 | 2.096256e-08\n",
            "  (Using basic Add-1 smoothing for unseen n-grams)\n",
            "  (Note: Raw probabilities are often extremely small; log probabilities are standard for comparison)\n",
            "Enter a sentence to calculate its probability (or press Enter to quit): Organized for the second consecutive year by the Four Wheel Drive Club ( FWDC ) of Sri Lanka and the Two Wheel Motor Racing Club ( TWMRC ) , the ... <p> Following its establishment in 2008 , Arinma Holdings @ @ @ @ @ @ @ @ @ @ post-war era in Sri Lanka . With a gamut of projects including the redevelopment of critical water supply initiatives that serve populations in arid areas of the nation , Arinma Holdings has developed global partnerships with stakeholders , both locally and ... <p> On 29th February , Saturday , Colombo 's popular jazz singer Gananath Dasanayaka with his band En Route will set the mood for an evening of good jazz at the OWSC in an atmosphere set for wining and dining . ' Cheers to Jazz ' will provide jazz lovers a Saturday evening of quality music that will include some popular numbers ... <p> International Institute of Health Science ( IIHS ) , Sri Lanka 's premier and leading healthcare education institute renowned for the high quality education provided for nurses in Sri Lanka recently entered into a collaboratory partnership with James Cook University of Australia . The partnership was one of key highlights as IIHS celebrated the \" Year of the Nurse and Midwife \" ... <p> LAUGFS International ( Pvt ) Ltd , the trading arm of LAUGFS Holdings , recently relocated to a brand new @ @ @ @ @ @ @ @ @ @ Sunethradevi Mawatha , Kohuwala , the new showroom provides a spacious and easy-to-access venue to better serve their loyal customers . As the authorized distributor for Yamaha and\n",
            "\n",
            "--- Probabilities for sentence: 'Organized for the second consecutive year by the Four Wheel Drive Club ( FWDC ) of Sri Lanka and the Two Wheel Motor Racing Club ( TWMRC ) , the ... <p> Following its establishment in 2008 , Arinma Holdings @ @ @ @ @ @ @ @ @ @ post-war era in Sri Lanka . With a gamut of projects including the redevelopment of critical water supply initiatives that serve populations in arid areas of the nation , Arinma Holdings has developed global partnerships with stakeholders , both locally and ... <p> On 29th February , Saturday , Colombo 's popular jazz singer Gananath Dasanayaka with his band En Route will set the mood for an evening of good jazz at the OWSC in an atmosphere set for wining and dining . ' Cheers to Jazz ' will provide jazz lovers a Saturday evening of quality music that will include some popular numbers ... <p> International Institute of Health Science ( IIHS ) , Sri Lanka 's premier and leading healthcare education institute renowned for the high quality education provided for nurses in Sri Lanka recently entered into a collaboratory partnership with James Cook University of Australia . The partnership was one of key highlights as IIHS celebrated the \" Year of the Nurse and Midwife \" ... <p> LAUGFS International ( Pvt ) Ltd , the trading arm of LAUGFS Holdings , recently relocated to a brand new @ @ @ @ @ @ @ @ @ @ Sunethradevi Mawatha , Kohuwala , the new showroom provides a spacious and easy-to-access venue to better serve their loyal customers . As the authorized distributor for Yamaha and' ---\n",
            "  Model      | Log Probability | Raw Probability (Approx.)\n",
            "-------------|-----------------|---------------------------\n",
            "  Unigram    |      -1771.0702 | 0.000000e+00\n",
            "  Bigram     |       -811.2421 | 0.000000e+00\n",
            "  Trigram    |       -398.4309 | 9.197095e-174\n",
            "  (Using basic Add-1 smoothing for unseen n-grams)\n",
            "  (Note: Raw probabilities are often extremely small; log probabilities are standard for comparison)\n",
            "Enter a sentence to calculate its probability (or press Enter to quit): World War II\n",
            "\n",
            "--- Probabilities for sentence: 'World War II' ---\n",
            "  Model      | Log Probability | Raw Probability (Approx.)\n",
            "-------------|-----------------|---------------------------\n",
            "  Unigram    |        -29.4027 | 1.700533e-13\n",
            "  Bigram     |        -27.1549 | 1.609863e-12\n",
            "  Trigram    |        -35.6333 | 3.347063e-16\n",
            "  (Using basic Add-1 smoothing for unseen n-grams)\n",
            "  (Note: Raw probabilities are often extremely small; log probabilities are standard for comparison)\n"
          ]
        }
      ],
      "source": [
        "# (Keep all the code above sentence_probability the same as the previous version)\n",
        "\n",
        "# --- 4. Calculate Sentence Probability (Refined to show Raw Probability) ---\n",
        "def sentence_probability(sentence, unigram_probs, bigram_probs, trigram_probs, unigram_counts, bigram_counts, unigram_prefix_counts):\n",
        "    \"\"\"\n",
        "    Calculates sentence log probability and raw probability using different models.\n",
        "    Handles unseen n-grams using basic Add-1 smoothing.\n",
        "    Prints results.\n",
        "    \"\"\"\n",
        "    results_log = {'unigram': -float('inf'), 'bigram': -float('inf'), 'trigram': -float('inf')}\n",
        "    results_raw = {'unigram': 0.0, 'bigram': 0.0, 'trigram': 0.0} # Initialize raw probs to 0\n",
        "\n",
        "    if not sentence or not sentence.strip():\n",
        "        print(\"Cannot calculate probability for empty sentence.\")\n",
        "        return\n",
        "\n",
        "    # Use the same tokenization as in preprocessing\n",
        "    words = re.findall(r'\\b\\w+\\'?\\w*\\b', sentence.lower())\n",
        "    if not words:\n",
        "        print(f\"Sentence '{sentence}' resulted in no tokens after processing.\")\n",
        "        return\n",
        "\n",
        "    # Prepare token lists with appropriate boundary markers\n",
        "    tokens_tri = [START_TOKEN, START_TOKEN] + words + [END_TOKEN]\n",
        "    tokens_bi = [START_TOKEN] + words + [END_TOKEN] # Standard bigram uses one start token\n",
        "\n",
        "    # --- Smoothing parameters (Add-1 style) ---\n",
        "    vocab_size = len(unigram_counts) if unigram_counts else 1\n",
        "    total_uni_count = sum(unigram_counts.values()) if unigram_counts else 0\n",
        "    den_uni_smooth = total_uni_count + vocab_size\n",
        "    min_uni_prob = 1.0 / den_uni_smooth if den_uni_smooth > 0 else 1e-10\n",
        "\n",
        "    # --- Unigram (Simple Independence Model) ---\n",
        "    log_prob_uni_current = 0.0\n",
        "    if total_uni_count > 0:\n",
        "        for word in words:\n",
        "            word_tuple = (word,)\n",
        "            prob = unigram_probs.get(word_tuple, min_uni_prob)\n",
        "            if prob <= 0: prob = 1e-10\n",
        "            log_prob_uni_current += math.log(prob)\n",
        "        results_log['unigram'] = log_prob_uni_current\n",
        "        # Calculate raw probability from log probability\n",
        "        # Use try-except for potential overflow with exp, though unlikely here\n",
        "        try:\n",
        "             results_raw['unigram'] = math.exp(log_prob_uni_current)\n",
        "        except OverflowError:\n",
        "             results_raw['unigram'] = 0.0 # Treat as effectively zero if exp overflows\n",
        "\n",
        "    # --- Bigram Model ---\n",
        "    log_prob_bi_current = 0.0\n",
        "    if len(tokens_bi) > 1 and unigram_prefix_counts and vocab_size > 0:\n",
        "        min_bi_prob_fallback = 1e-10\n",
        "        for i in range(len(tokens_bi) - 1):\n",
        "            bg = tuple(tokens_bi[i:i+2])\n",
        "            prob = bigram_probs.get(bg, 0.0)\n",
        "            if prob == 0.0:\n",
        "                prefix_token = bg[0]\n",
        "                prefix_count = unigram_prefix_counts.get(prefix_token, 0)\n",
        "                den_bi_smooth = prefix_count + vocab_size\n",
        "                prob = 1.0 / den_bi_smooth if den_bi_smooth > 0 else min_bi_prob_fallback\n",
        "            if prob <= 0: prob = 1e-10\n",
        "            log_prob_bi_current += math.log(prob)\n",
        "        results_log['bigram'] = log_prob_bi_current\n",
        "        try:\n",
        "            results_raw['bigram'] = math.exp(log_prob_bi_current)\n",
        "        except OverflowError:\n",
        "             results_raw['bigram'] = 0.0\n",
        "\n",
        "    # --- Trigram Model ---\n",
        "    log_prob_tri_current = 0.0\n",
        "    if len(tokens_tri) > 2 and bigram_counts and vocab_size > 0:\n",
        "        min_tri_prob_fallback = 1e-10\n",
        "        for i in range(len(tokens_tri) - 2):\n",
        "            tg = tuple(tokens_tri[i:i+3])\n",
        "            prob = trigram_probs.get(tg, 0.0)\n",
        "            if prob == 0.0:\n",
        "                prefix_bigram = tg[:-1]\n",
        "                prefix_count = bigram_counts.get(prefix_bigram, 0)\n",
        "                den_tri_smooth = prefix_count + vocab_size\n",
        "                prob = 1.0 / den_tri_smooth if den_tri_smooth > 0 else min_tri_prob_fallback\n",
        "            if prob <= 0: prob = 1e-10\n",
        "            log_prob_tri_current += math.log(prob)\n",
        "        results_log['trigram'] = log_prob_tri_current\n",
        "        try:\n",
        "             results_raw['trigram'] = math.exp(log_prob_tri_current)\n",
        "        except OverflowError:\n",
        "             results_raw['trigram'] = 0.0\n",
        "\n",
        "\n",
        "    # --- Print the results ---\n",
        "    print(f\"\\n--- Probabilities for sentence: '{sentence}' ---\")\n",
        "    print(f\"  Model      | Log Probability | Raw Probability (Approx.)\")\n",
        "    print(f\"-------------|-----------------|---------------------------\")\n",
        "    print(f\"  Unigram    | {results_log['unigram']:15.4f} | {results_raw['unigram']:.6e}\")\n",
        "    print(f\"  Bigram     | {results_log['bigram']:15.4f} | {results_raw['bigram']:.6e}\")\n",
        "    print(f\"  Trigram    | {results_log['trigram']:15.4f} | {results_raw['trigram']:.6e}\")\n",
        "    print(f\"  (Using basic Add-1 smoothing for unseen n-grams)\")\n",
        "    print(f\"  (Note: Raw probabilities are often extremely small; log probabilities are standard for comparison)\")\n",
        "\n",
        "# --- Main Execution ---\n",
        "# (Keep the main execution block the same, calling this updated sentence_probability function)\n",
        "# ... (rest of the script from previous answer) ...\n",
        "\n",
        "print(\"--- Starting N-gram Model Building ---\")\n",
        "\n",
        "# 1. Preprocess Text\n",
        "tokens = preprocess_text(file_path)\n",
        "\n",
        "if tokens:\n",
        "    # 2. Count N-grams\n",
        "    print(\"\\n--- Counting N-grams ---\")\n",
        "    unigram_counts = build_ngram_counts(tokens, 1)\n",
        "    bigram_counts = build_ngram_counts(tokens, 2)\n",
        "    trigram_counts = build_ngram_counts(tokens, 3)\n",
        "\n",
        "    # Check if counts were successful\n",
        "    if not unigram_counts or not bigram_counts or not trigram_counts:\n",
        "        print(\"\\nError: Failed to generate n-gram counts. Cannot proceed.\")\n",
        "    else:\n",
        "        print(f\"Total Unigrams (tokens): {sum(unigram_counts.values())}\")\n",
        "        print(f\"Unique Unigrams: {len(unigram_counts)}\")\n",
        "        print(f\"Total Bigrams: {sum(bigram_counts.values())}\")\n",
        "        print(f\"Unique Bigrams: {len(bigram_counts)}\")\n",
        "        print(f\"Total Trigrams: {sum(trigram_counts.values())}\")\n",
        "        print(f\"Unique Trigrams: {len(trigram_counts)}\")\n",
        "\n",
        "        # --- Display Sample Counts ---\n",
        "        print(\"\\n--- Sample N-gram Counts ---\")\n",
        "        # Ensure counts are not empty before calling most_common\n",
        "        if unigram_counts: print(\"Top 5 Unigrams:\", unigram_counts.most_common(5))\n",
        "        if bigram_counts: print(\"Top 5 Bigrams:\", bigram_counts.most_common(5))\n",
        "        if trigram_counts: print(\"Top 5 Trigrams:\", trigram_counts.most_common(5))\n",
        "\n",
        "        # 3. Calculate Probabilities (MLE)\n",
        "        print(\"\\n--- Calculating Probabilities (MLE) ---\")\n",
        "\n",
        "        total_token_count_for_unigrams = sum(unigram_counts.values())\n",
        "        unigram_prefix_counts_for_bigrams = Counter(token for token in tokens)\n",
        "\n",
        "        unigram_probs = calculate_ngram_probabilities(unigram_counts, None, total_token_count_for_unigrams, n=1)\n",
        "        bigram_probs = calculate_ngram_probabilities(bigram_counts, unigram_prefix_counts_for_bigrams, None, n=2)\n",
        "        trigram_probs = calculate_ngram_probabilities(trigram_counts, bigram_counts, None, n=3)\n",
        "\n",
        "        # Check if probability dictionaries were created\n",
        "        if not unigram_probs or not bigram_probs or not trigram_probs:\n",
        "             print(\"\\nError: Failed to calculate probabilities. Cannot proceed.\")\n",
        "        else:\n",
        "            # --- Display Sample Probabilities ---\n",
        "            print(\"\\n--- Sample N-gram Probabilities (MLE) ---\")\n",
        "            # Unigrams\n",
        "            print(\"Unigram Probabilities:\")\n",
        "            if unigram_counts:\n",
        "              sample_unigrams = [ug for ug, count in unigram_counts.most_common(5)]\n",
        "              for ug in sample_unigrams:\n",
        "                  # Use .get to avoid KeyError if prob somehow missing\n",
        "                  print(f\"  P({ug[0]}) = {unigram_probs.get(ug, 0.0):.6f}\")\n",
        "            else: print(\"  No unigrams to calculate probabilities for.\")\n",
        "\n",
        "            # Bigrams\n",
        "            print(\"\\nBigram Probabilities:\")\n",
        "            if bigram_counts:\n",
        "              sample_bigrams = [bg for bg, count in bigram_counts.most_common(5)]\n",
        "              for bg in sample_bigrams:\n",
        "                  prefix_uni = bg[0]\n",
        "                  print(f\"  P({bg[1]} | {prefix_uni}) = {bigram_probs.get(bg, 0.0):.6f}  (Count({bg})={bigram_counts.get(bg,0)}, Count({prefix_uni})={unigram_prefix_counts_for_bigrams.get(prefix_uni, 0)})\")\n",
        "            else: print(\"  No bigrams to calculate probabilities for.\")\n",
        "\n",
        "\n",
        "            # Trigrams\n",
        "            print(\"\\nTrigram Probabilities:\")\n",
        "            if trigram_counts:\n",
        "              sample_trigrams = [tg for tg, count in trigram_counts.most_common(5)]\n",
        "              for tg in sample_trigrams:\n",
        "                  prefix_bi = tg[:-1]\n",
        "                  print(f\"  P({tg[2]} | {tg[0]}, {tg[1]}) = {trigram_probs.get(tg, 0.0):.6f}  (Count({tg})={trigram_counts.get(tg,0)}, Count({prefix_bi})={bigram_counts.get(prefix_bi, 0)})\")\n",
        "            else: print(\"  No trigrams to calculate probabilities for.\")\n",
        "\n",
        "            print(\"\\n--- Model Building Complete ---\")\n",
        "\n",
        "            # --- Calculate Probability for Example Sentences ---\n",
        "            print(\"\\n--- Calculating Probabilities for Example Sentences ---\")\n",
        "            test_sentence_1 = \"the government last july called the energy sector\" # Likely exists\n",
        "            test_sentence_2 = \"this is a completely random sentence\" # Likely has unseen n-grams\n",
        "            sentence_probability(test_sentence_1, unigram_probs, bigram_probs, trigram_probs, unigram_counts, bigram_counts, unigram_prefix_counts_for_bigrams)\n",
        "            sentence_probability(test_sentence_2, unigram_probs, bigram_probs, trigram_probs, unigram_counts, bigram_counts, unigram_prefix_counts_for_bigrams)\n",
        "\n",
        "\n",
        "            # --- USER INPUT LOOP ---\n",
        "            print(\"\\n--- Calculate Probability for User Input ---\")\n",
        "            while True:\n",
        "                try:\n",
        "                    user_sentence = input(\"Enter a sentence to calculate its probability (or press Enter to quit): \")\n",
        "                    if not user_sentence.strip():\n",
        "                        print(\"Exiting.\")\n",
        "                        break\n",
        "                    # Call the function - it now handles printing internally\n",
        "                    sentence_probability(user_sentence,\n",
        "                                         unigram_probs,\n",
        "                                         bigram_probs,\n",
        "                                         trigram_probs,\n",
        "                                         unigram_counts,\n",
        "                                         bigram_counts,\n",
        "                                         unigram_prefix_counts_for_bigrams)\n",
        "                except EOFError:\n",
        "                     print(\"\\nExiting due to EOF.\")\n",
        "                     break\n",
        "                except KeyboardInterrupt:\n",
        "                     print(\"\\nExiting due to user interrupt.\")\n",
        "                     break\n",
        "                except Exception as e:\n",
        "                     print(f\"\\nAn error occurred during input/calculation: {e}\")\n",
        "                     # Optional: break or continue\n",
        "                     # break\n",
        "\n",
        "# --- Fallback if issues occurred earlier ---\n",
        "else:\n",
        "    print(\"\\nCould not process the file or build models. Input loop skipped.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}