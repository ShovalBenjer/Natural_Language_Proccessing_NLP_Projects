{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShovalBenjer/Natural_Language_Proccessing_NLP_Projects/blob/main/LSTM_NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies as needed:\n",
        "!pip install kagglehub[pandas-datasets] torch torchvision torchaudio plotnine tqdm scikit-learn\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import os\n",
        "import re\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "from tqdm.auto import tqdm # For progress bars\n",
        "\n",
        "# For plotting\n",
        "from plotnine import ggplot, aes, geom_line, labs, theme_minimal\n",
        "\n",
        "# For reproducibility\n",
        "import random\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "MHcmiRTZgywB",
        "outputId": "7b27c0f5-4e0e-4626-8179-0de67c25d749",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: plotnine in /usr/local/lib/python3.11/dist-packages (0.14.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: kagglehub[pandas-datasets] in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub[pandas-datasets]) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub[pandas-datasets]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub[pandas-datasets]) (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from kagglehub[pandas-datasets]) (2.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: matplotlib>=3.8.0 in /usr/local/lib/python3.11/dist-packages (from plotnine) (3.10.0)\n",
            "Requirement already satisfied: mizani~=0.13.0 in /usr/local/lib/python3.11/dist-packages (from plotnine) (0.13.5)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from plotnine) (1.15.3)\n",
            "Requirement already satisfied: statsmodels>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from plotnine) (0.14.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8.0->plotnine) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8.0->plotnine) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8.0->plotnine) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8.0->plotnine) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8.0->plotnine) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8.0->plotnine) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->kagglehub[pandas-datasets]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->kagglehub[pandas-datasets]) (2025.2)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.14.0->plotnine) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub[pandas-datasets]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub[pandas-datasets]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub[pandas-datasets]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub[pandas-datasets]) (2025.4.26)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.8.0->plotnine) (1.17.0)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Load Data ---\n",
        "print(\"Loading dataset...\")\n",
        "# Let's pick one of the comment files. If file_path is empty, it loads CommentsApril2017.csv by default.\n",
        "# For a more robust solution, you might want to load and concatenate multiple files.\n",
        "# For this example, we'll use the default or specify one for clarity.\n",
        "# file_path = \"CommentsApril2017.csv\" # Example specific file\n",
        "file_path = \"\" # Let KaggleHub pick the default for simplicity\n",
        "\n",
        "try:\n",
        "    df = kagglehub.load_dataset(\n",
        "      KaggleDatasetAdapter.PANDAS,\n",
        "      \"aashita/nyt-comments\",\n",
        "      file_path=file_path,\n",
        "    )\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    print(\"First 5 records:\", df.head())\n",
        "    # Select the comment text column\n",
        "    # Based on typical Kaggle datasets, it might be 'commentBody' or similar\n",
        "    # Let's inspect columns if 'commentBody' isn't present\n",
        "    if 'commentBody' in df.columns:\n",
        "        corpus_raw = df['commentBody'].dropna().astype(str).tolist()\n",
        "    elif 'commentText' in df.columns:\n",
        "        corpus_raw = df['commentText'].dropna().astype(str).tolist()\n",
        "    else:\n",
        "        # Fallback: try to find a text-like column\n",
        "        text_cols = [col for col in df.columns if df[col].dtype == 'object']\n",
        "        if not text_cols:\n",
        "            raise ValueError(\"No suitable text column found in the dataset.\")\n",
        "        print(f\"Warning: 'commentBody' or 'commentText' not found. Using first object column: {text_cols[0]}\")\n",
        "        corpus_raw = df[text_cols[0]].dropna().astype(str).tolist()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\")\n",
        "    print(\"Using dummy data for demonstration purposes.\")\n",
        "    corpus_raw = [\n",
        "        \"This is the first comment about politics.\",\n",
        "        \"I agree with the previous sentiment.\",\n",
        "        \"Another comment regarding the current events.\",\n",
        "        \"Let's talk about technology and AI.\",\n",
        "        \"The future of AI is fascinating and scary.\"\n",
        "    ]\n",
        "\n",
        "# For faster demonstration, let's use a smaller subset\n",
        "MAX_SAMPLES = 5000  # Adjust as needed for your hardware\n",
        "if len(corpus_raw) > MAX_SAMPLES:\n",
        "    print(f\"Using a subset of {MAX_SAMPLES} comments for faster processing.\")\n",
        "    corpus_raw = random.sample(corpus_raw, MAX_SAMPLES)\n",
        "else:\n",
        "    print(f\"Using all {len(corpus_raw)} available comments.\")"
      ],
      "metadata": {
        "id": "COtOzMjwg7Hk",
        "outputId": "1d12a4da-4617-4365-ff2e-88b7e34097e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n",
            "Error loading dataset: load_dataset() got an unexpected keyword argument 'file_path'\n",
            "Using dummy data for demonstration purposes.\n",
            "Using all 5 available comments.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Preprocess Data ---\n",
        "print(\"\\nPreprocessing data...\")\n",
        "\n",
        "def clean_text(txt):\n",
        "    txt = txt.lower()\n",
        "    # Remove URLs\n",
        "    txt = re.sub(r'http\\S+|www\\S+|https\\S+', '', txt, flags=re.MULTILINE)\n",
        "    # Remove user @ references and #hashtags\n",
        "    txt = re.sub(r'\\@\\w+|\\#','', txt)\n",
        "    # Remove punctuation\n",
        "    txt = \"\".join(v for v in txt if v not in string.punctuation.replace(\"'\", \"\")) # Keep apostrophes for now\n",
        "    txt = txt.replace(\"’\", \"'\") # Normalize apostrophes\n",
        "    # Remove numbers (optional, depends on whether you want numbers in your LM)\n",
        "    # txt = re.sub(r'\\d+', '', txt)\n",
        "    # Remove emojis (a basic way, might need a more comprehensive library for full coverage)\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    txt = emoji_pattern.sub(r'', txt)\n",
        "    txt = txt.encode(\"utf8\").decode(\"ascii\", 'ignore') # Handle residual non-ascii\n",
        "    txt = re.sub(r'\\s+', ' ', txt).strip() # Remove extra whitespace\n",
        "    return txt\n",
        "\n",
        "corpus_cleaned = [clean_text(text) for text in tqdm(corpus_raw, desc=\"Cleaning text\") if clean_text(text)]\n",
        "corpus_cleaned = [text for text in corpus_cleaned if len(text.split()) > 2] # Keep sentences with at least 3 words\n",
        "\n",
        "print(f\"Sample cleaned comment: {corpus_cleaned[0] if corpus_cleaned else 'N/A'}\")\n",
        "\n",
        "# Tokenization and Vocabulary\n",
        "class Tokenizer:\n",
        "    def __init__(self, oov_token=\"<unk>\"):\n",
        "        self.word_to_idx = {}\n",
        "        self.idx_to_word = {}\n",
        "        self.oov_token = oov_token\n",
        "        self.add_word(oov_token) # Add OOV token\n",
        "\n",
        "    def fit_on_texts(self, texts):\n",
        "        word_counts = {}\n",
        "        for text in texts:\n",
        "            for word in text.split():\n",
        "                word_counts[word] = word_counts.get(word, 0) + 1\n",
        "\n",
        "        # Sort words by frequency for consistent indexing (optional but good practice)\n",
        "        sorted_words = sorted(word_counts.keys(), key=lambda x: word_counts[x], reverse=True)\n",
        "\n",
        "        for word in sorted_words:\n",
        "            self.add_word(word)\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word_to_idx:\n",
        "            idx = len(self.word_to_idx)\n",
        "            self.word_to_idx[word] = idx\n",
        "            self.idx_to_word[idx] = word\n",
        "\n",
        "    def texts_to_sequences(self, texts):\n",
        "        sequences = []\n",
        "        oov_idx = self.word_to_idx[self.oov_token]\n",
        "        for text in texts:\n",
        "            seq = [self.word_to_idx.get(word, oov_idx) for word in text.split()]\n",
        "            sequences.append(seq)\n",
        "        return sequences\n",
        "\n",
        "    def sequences_to_texts(self, sequences):\n",
        "        texts = []\n",
        "        for seq in sequences:\n",
        "            words = [self.idx_to_word.get(idx, self.oov_token) for idx in seq]\n",
        "            texts.append(\" \".join(words))\n",
        "        return texts\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.word_to_idx)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus_cleaned)\n",
        "total_words = tokenizer.vocab_size\n",
        "print(f\"Total unique words in vocabulary: {total_words}\")\n",
        "\n",
        "# Generate N-gram sequences\n",
        "input_sequences = []\n",
        "for line in tqdm(corpus_cleaned, desc=\"Generating n-gram sequences\"):\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    if not token_list: continue # Skip empty lines after tokenization\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "if not input_sequences:\n",
        "    raise ValueError(\"No input sequences generated. Check corpus cleaning and tokenization, or increase MAX_SAMPLES.\")\n",
        "\n",
        "print(f\"Number of n-gram sequences: {len(input_sequences)}\")\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "print(f\"Max sequence length: {max_sequence_len}\")\n",
        "\n",
        "# Pad sequences and create predictors/labels\n",
        "# Convert to PyTorch tensors directly\n",
        "predictors_list = []\n",
        "labels_list = []\n",
        "\n",
        "for seq in input_sequences:\n",
        "    predictors_list.append(torch.tensor(seq[:-1], dtype=torch.long))\n",
        "    labels_list.append(torch.tensor(seq[-1], dtype=torch.long))\n",
        "\n",
        "# Pad predictor sequences\n",
        "# The input to LSTM expects (seq_len, batch, input_size) or (batch, seq_len, input_size) if batch_first=True\n",
        "# Embedding layer will take care of converting token indices to vectors.\n",
        "# We need to pad the sequences of token indices.\n",
        "# `pad_sequence` expects a list of Tensors and pads them to the length of the longest Tensor.\n",
        "# It pads with 0 by default, which is fine if 0 is not a valid token index or if we use padding_idx in Embedding.\n",
        "# Our tokenizer starts indexing from 0, so 0 is a valid token (<unk>).\n",
        "# We can add a <pad> token or use `padding_value` if necessary. Let's assume 0 for <unk> is okay for now.\n",
        "# Or, better, make tokenizer reserve 0 for padding. Let's adjust tokenizer slightly.\n",
        "\n",
        "class TokenizerWithPadding(Tokenizer):\n",
        "    def __init__(self, oov_token=\"<unk>\", pad_token=\"<pad>\"):\n",
        "        super().__init__(oov_token)\n",
        "        self.pad_token = pad_token\n",
        "        # Ensure pad token is 0, and OOV is 1, then actual words\n",
        "        self.word_to_idx = {pad_token: 0, oov_token: 1}\n",
        "        self.idx_to_word = {0: pad_token, 1: oov_token}\n",
        "        self.pad_idx = 0\n",
        "        self.oov_idx = 1\n",
        "        self._next_idx = 2 # Next available index for new words\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word_to_idx:\n",
        "            self.word_to_idx[word] = self._next_idx\n",
        "            self.idx_to_word[self._next_idx] = word\n",
        "            self._next_idx += 1\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return self._next_idx # Correct vocab size calculation\n",
        "\n",
        "# Re-tokenize with padding-aware tokenizer\n",
        "tokenizer = TokenizerWithPadding()\n",
        "tokenizer.fit_on_texts(corpus_cleaned)\n",
        "total_words = tokenizer.vocab_size # This is actually vocab_size + 1 if we count padding\n",
        "print(f\"Total unique words in vocabulary (incl. <pad>, <unk>): {total_words}\")\n",
        "\n",
        "input_sequences = []\n",
        "for line in tqdm(corpus_cleaned, desc=\"Re-generating n-gram sequences\"):\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    # Filter out sequences that are only <unk> or too short\n",
        "    if not token_list or all(t == tokenizer.oov_idx for t in token_list): continue\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        # Ensure n_gram_sequence has at least one non-pad, non-unk token before the target\n",
        "        # This means the predictor part (n_gram_sequence[:-1]) should not be empty\n",
        "        if len(n_gram_sequence[:-1]) > 0:\n",
        "            input_sequences.append(n_gram_sequence)\n",
        "\n",
        "\n",
        "if not input_sequences:\n",
        "    raise ValueError(\"No input sequences generated after re-tokenization. Check data or sampling.\")\n",
        "\n",
        "max_sequence_len_model_input = max([len(x[:-1]) for x in input_sequences if len(x[:-1]) > 0])\n",
        "if max_sequence_len_model_input == 0:\n",
        "    raise ValueError(\"Max sequence length for predictors is 0. All n-grams might be too short.\")\n",
        "print(f\"Max predictor sequence length: {max_sequence_len_model_input}\")\n",
        "\n",
        "\n",
        "predictors_list = []\n",
        "labels_list = []\n",
        "for seq in input_sequences:\n",
        "    predictor_part = seq[:-1]\n",
        "    label_part = seq[-1]\n",
        "    if len(predictor_part) > 0: # Ensure predictor is not empty\n",
        "        predictors_list.append(torch.tensor(predictor_part, dtype=torch.long))\n",
        "        labels_list.append(torch.tensor(label_part, dtype=torch.long))\n",
        "\n",
        "# Pad predictor sequences\n",
        "# `batch_first=True` means pad_sequence will output (batch_size, max_len)\n",
        "padded_predictors = pad_sequence(predictors_list, batch_first=True, padding_value=tokenizer.pad_idx)\n",
        "labels_tensor = torch.stack(labels_list) # Stack labels into a single tensor\n",
        "\n",
        "print(f\"Shape of padded predictors: {padded_predictors.shape}\")\n",
        "print(f\"Shape of labels: {labels_tensor.shape}\")"
      ],
      "metadata": {
        "id": "WtKOmQN1hBce",
        "outputId": "47416b0f-2992-4db2-a4e9-4d9d562fb667",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542,
          "referenced_widgets": [
            "22b75898c5f940c1ab9de5d22728df9d",
            "d3929b4ba07c4b5fbe5f6856f9c7b254",
            "cda9ddde4523466e9593f1b4d8b0eadf",
            "baaf7bd514354bf0922d83daa25e4bc6",
            "824dc5dfb494465cb982b8ae9875114b",
            "add39ed0cf2c43579908e0aecff8e1ce",
            "fe9e7807b29f4b3ba4621a7fe675fcb2",
            "c6400845699f4f63bb44644ee827ced2",
            "d449f64812ca4111a6bf9fa1d03b69f8",
            "abe52347b96f4652bd25c567964a62b7",
            "f81dbcc1a92547afb79997efbf49eece",
            "bac79d731ce64460aec3d131b630814e",
            "a38a786d330f45fc8dc620a098633e12",
            "227eb829838e4aeaa3a750f3cccae923",
            "e785f78610e8406081a69a3b00705547",
            "d429391ea0f74ea49b7bd219760c2d74",
            "f370388c768c4ce1bef1a53f6d06f409",
            "a4c109de42a2440db912e944f6f80ced",
            "41ea770492cd485ebb91786c0a15d90d",
            "91679a6100a243e0b259b05a74259a93",
            "cc8846d9b8be4b44a54869252fb8179b",
            "270e2244146f469db8158a1780e1c365"
          ]
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preprocessing data...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Cleaning text:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "22b75898c5f940c1ab9de5d22728df9d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample cleaned comment: this is the first comment about politics\n",
            "Total unique words in vocabulary: 26\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating n-gram sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d3929b4ba07c4b5fbe5f6856f9c7b254"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of n-gram sequences: 28\n",
            "Max sequence length: 8\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'TokenizerWithPadding' object has no attribute '_next_idx'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-d114fda709d3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;31m# Re-tokenize with padding-aware tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizerWithPadding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_cleaned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0mtotal_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[0;31m# This is actually vocab_size + 1 if we count padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-d114fda709d3>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, oov_token, pad_token)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTokenizerWithPadding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moov_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"<unk>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"<pad>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moov_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# Ensure pad token is 0, and OOV is 1, then actual words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-d114fda709d3>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, oov_token)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx_to_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moov_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moov_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moov_token\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Add OOV token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-d114fda709d3>\u001b[0m in \u001b[0;36madd_word\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_to_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_to_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx_to_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'TokenizerWithPadding' object has no attribute '_next_idx'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Create PyTorch Dataset and DataLoader ---\n",
        "class CommentDataset(Dataset):\n",
        "    def __init__(self, predictors, labels):\n",
        "        self.predictors = predictors\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.predictors[idx], self.labels[idx]\n",
        "\n",
        "dataset = CommentDataset(padded_predictors, labels_tensor)\n",
        "\n",
        "# Split into training and validation (optional, but good practice)\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "if val_size == 0 and train_size > 0 : # handle tiny datasets for testing\n",
        "    train_dataset = dataset\n",
        "    val_dataset = dataset # use train as val if val_size is 0\n",
        "else:\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "\n",
        "BATCH_SIZE = 128 # Can be tuned\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "if val_size > 0:\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "else: # if val_size is 0, val_loader can be None or point to train_loader for simplicity in eval loop\n",
        "    val_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "\n",
        "# --- 4. Define LSTM Model in PyTorch ---\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, padding_idx):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers,\n",
        "                            batch_first=True, dropout=dropout_rate if num_layers > 1 else 0)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len)\n",
        "        embedded = self.embedding(x)\n",
        "        # embedded shape: (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        # lstm_out shape: (batch_size, seq_len, hidden_dim)\n",
        "\n",
        "        # We only want the output from the last time step\n",
        "        last_lstm_out = lstm_out[:, -1, :]\n",
        "        # last_lstm_out shape: (batch_size, hidden_dim)\n",
        "\n",
        "        out = self.dropout(last_lstm_out)\n",
        "        out = self.fc(out)\n",
        "        # out shape: (batch_size, vocab_size)\n",
        "        return out\n",
        "\n",
        "# Hyperparameters\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 150\n",
        "NUM_LAYERS = 2 # Using 2 LSTM layers\n",
        "DROPOUT_RATE = 0.2\n",
        "LEARNING_RATE = 0.001 # Adam's default is 0.001\n",
        "EPOCHS = 20 # Start with a smaller number for quick testing\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = LanguageModel(\n",
        "    vocab_size=total_words,\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        "    padding_idx=tokenizer.pad_idx\n",
        ").to(device)\n",
        "\n",
        "print(model)\n",
        "# Sanity check one batch\n",
        "try:\n",
        "    sample_x, sample_y = next(iter(train_loader))\n",
        "    sample_x, sample_y = sample_x.to(device), sample_y.to(device)\n",
        "    output = model(sample_x)\n",
        "    print(\"Sample output shape:\", output.shape) # Expected: (BATCH_SIZE, total_words)\n",
        "    print(\"Sample target shape:\", sample_y.shape) # Expected: (BATCH_SIZE)\n",
        "except StopIteration:\n",
        "    print(\"Train loader is empty. Cannot perform sanity check.\")\n",
        "    # This can happen if MAX_SAMPLES is too small or data cleaning is too aggressive.\n",
        "\n",
        "# --- 5. Training Loop ---\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_idx) # Ignore padding in loss calculation\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "history = {'epoch': [], 'train_loss': [], 'val_loss': [], 'perplexity': []}\n",
        "\n",
        "print(\"\\nStarting training...\")\n",
        "if not train_loader:\n",
        "    print(\"Skipping training as no data is available in train_loader.\")\n",
        "else:\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        epoch_train_loss = 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Training]\", leave=False)\n",
        "        for batch_predictors, batch_labels in progress_bar:\n",
        "            batch_predictors, batch_labels = batch_predictors.to(device), batch_labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_predictors)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_train_loss += loss.item()\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
        "        history['epoch'].append(epoch + 1)\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        epoch_val_loss = 0\n",
        "        if val_loader and len(val_loader) > 0: # Check if val_loader is not empty\n",
        "            with torch.no_grad():\n",
        "                progress_bar_val = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Validation]\", leave=False)\n",
        "                for batch_predictors, batch_labels in progress_bar_val:\n",
        "                    batch_predictors, batch_labels = batch_predictors.to(device), batch_labels.to(device)\n",
        "                    outputs = model(batch_predictors)\n",
        "                    loss = criterion(outputs, batch_labels)\n",
        "                    epoch_val_loss += loss.item()\n",
        "                    progress_bar_val.set_postfix(loss=loss.item())\n",
        "            avg_val_loss = epoch_val_loss / len(val_loader)\n",
        "            perplexity = np.exp(avg_val_loss) # Perplexity = e^(avg_cross_entropy_loss)\n",
        "        else: # Handle case with no validation data\n",
        "            avg_val_loss = float('nan')\n",
        "            perplexity = float('nan')\n",
        "\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "        history['perplexity'].append(perplexity)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f} - Perplexity: {perplexity:.2f}\")"
      ],
      "metadata": {
        "id": "VLag2AKRhN5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "lIYdn1woOS1n",
        "outputId": "c406f9af-a9a2-4ea9-a5f4-2aa39a52bf12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-693208dbfad4>:10: DeprecationWarning: load_dataset is deprecated and will be removed in a future version.\n",
            "  df = kagglehub.load_dataset(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unsupported file extension: ''. Supported file extensions are: .csv, .tsv, .json, .jsonl, .xml, .parquet, .feather, .sqlite, .sqlite3, .db, .db3, .s3db, .dl3, .xls, .xlsx, .xlsm, .xlsb, .odf, .ods, .odt",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-693208dbfad4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Load the latest version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m df = kagglehub.load_dataset(\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0mKaggleDatasetAdapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPANDAS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;34m\"aashita/nyt-comments\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglehub/datasets.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(adapter, handle, path, pandas_kwargs, sql_query, hf_kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;34m\"load_dataset is deprecated and will be removed in a future version.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     )\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpandas_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpandas_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msql_query\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msql_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhf_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhf_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglehub/datasets.py\u001b[0m in \u001b[0;36mdataset_load\u001b[0;34m(adapter, handle, path, pandas_kwargs, sql_query, hf_kwargs, polars_frame_type, polars_kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mkagglehub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpandas_datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             return kagglehub.pandas_datasets.load_pandas_dataset(\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpandas_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpandas_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msql_query\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msql_query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglehub/pandas_datasets.py\u001b[0m in \u001b[0;36mload_pandas_dataset\u001b[0;34m(handle, path, pandas_kwargs, sql_query)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mpandas_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpandas_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpandas_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mfile_extension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mread_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_read_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_extension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msql_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;31m# Now that everything has been validated, we can start downloading and processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglehub/pandas_datasets.py\u001b[0m in \u001b[0;36m_validate_read_function\u001b[0;34m(file_extension, sql_query)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;34mf\"Supported file extensions are: {', '.join(SUPPORTED_READ_FUNCTIONS_BY_EXTENSION.keys())}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         )\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextension_error_message\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mread_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSUPPORTED_READ_FUNCTIONS_BY_EXTENSION\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_extension\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unsupported file extension: ''. Supported file extensions are: .csv, .tsv, .json, .jsonl, .xml, .parquet, .feather, .sqlite, .sqlite3, .db, .db3, .s3db, .dl3, .xls, .xlsx, .xlsm, .xlsb, .odf, .ods, .odt"
          ]
        }
      ],
      "source": [
        "# --- 6. Text Generation Function ---\n",
        "def generate_text(seed_text, next_words, model, tokenizer, max_sequence_len_model_input, device, temperature=1.0):\n",
        "    model.eval()\n",
        "    generated_text = seed_text\n",
        "\n",
        "    for _ in range(next_words):\n",
        "        # Tokenize current text\n",
        "        token_list = tokenizer.texts_to_sequences([generated_text.lower()])[0]\n",
        "\n",
        "        # Pad sequence (take only the last `max_sequence_len_model_input` tokens)\n",
        "        if len(token_list) > max_sequence_len_model_input:\n",
        "            token_list = token_list[-max_sequence_len_model_input:]\n",
        "\n",
        "        # Convert to tensor and add batch dimension\n",
        "        input_tensor = torch.tensor([token_list], dtype=torch.long).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(input_tensor) # Shape: (1, vocab_size)\n",
        "\n",
        "        # Apply temperature to logits\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "\n",
        "        # Sample from the distribution or take argmax\n",
        "        # For more diverse generation, use sampling:\n",
        "        # top_i = torch.multinomial(output_dist, 1)[0]\n",
        "\n",
        "        # For deterministic (but potentially repetitive) generation, use argmax:\n",
        "        if temperature == 0.0: # Pure argmax\n",
        "            predicted_idx = torch.argmax(output, dim=1).item()\n",
        "        else: # Sampling with temperature\n",
        "            predicted_idx = torch.multinomial(output_dist, 1)[0].item()\n",
        "\n",
        "\n",
        "        # Handle OOV or PAD prediction if they occur (less likely with good training)\n",
        "        if predicted_idx == tokenizer.pad_idx and tokenizer.pad_token != \"<pad_is_word>\": # Avoid predicting padding\n",
        "            # Fallback: predict the next most likely non-pad token\n",
        "            sorted_preds = torch.argsort(output, dim=1, descending=True)\n",
        "            for idx_val in sorted_preds[0]:\n",
        "                if idx_val.item() != tokenizer.pad_idx:\n",
        "                    predicted_idx = idx_val.item()\n",
        "                    break\n",
        "\n",
        "        output_word = tokenizer.idx_to_word.get(predicted_idx, tokenizer.oov_token)\n",
        "\n",
        "        if output_word == tokenizer.oov_token: # Don't append <unk> if it's just a placeholder\n",
        "            continue # Or break, or try another prediction\n",
        "\n",
        "        generated_text += \" \" + output_word\n",
        "\n",
        "    return generated_text.title() # Capitalize like the Keras example\n",
        "\n",
        "print(\"\\n--- Text Generation Examples ---\")\n",
        "if not train_loader:\n",
        "    print(\"Skipping generation as model was not trained.\")\n",
        "else:\n",
        "    try:\n",
        "        seed1 = \"the president said\"\n",
        "        seed2 = \"new york is\"\n",
        "        seed3 = \"climate change will\"\n",
        "\n",
        "        print(f\"Seed: '{seed1}'\")\n",
        "        print(\"Generated (temp=0.7):\", generate_text(seed1, 10, model, tokenizer, max_sequence_len_model_input, device, temperature=0.7))\n",
        "        print(\"Generated (temp=1.0):\", generate_text(seed1, 10, model, tokenizer, max_sequence_len_model_input, device, temperature=1.0))\n",
        "        print(\"Generated (argmax):\", generate_text(seed1, 10, model, tokenizer, max_sequence_len_model_input, device, temperature=0.0)) # temperature 0 for argmax\n",
        "\n",
        "        print(f\"\\nSeed: '{seed2}'\")\n",
        "        print(\"Generated (temp=0.7):\", generate_text(seed2, 10, model, tokenizer, max_sequence_len_model_input, device, temperature=0.7))\n",
        "\n",
        "        print(f\"\\nSeed: '{seed3}'\")\n",
        "        print(\"Generated (temp=0.7):\", generate_text(seed3, 10, model, tokenizer, max_sequence_len_model_input, device, temperature=0.7))\n",
        "    except Exception as e:\n",
        "        print(f\"Error during text generation: {e}\")\n",
        "        print(\"This might be due to a very small vocabulary or issues with sequence lengths.\")\n",
        "\n",
        "\n",
        "# --- 7. Evaluation (Perplexity is already calculated during training) ---\n",
        "# Qualitative evaluation is looking at the generated text.\n",
        "# Quantitative:\n",
        "# - Perplexity on a test set (calculated as exp(average cross-entropy loss on test set)).\n",
        "# - BLEU scores if comparing to reference continuations (more for machine translation but adaptable).\n",
        "\n",
        "print(\"\\n--- Evaluation ---\")\n",
        "print(\"Perplexity on the validation set is tracked during training.\")\n",
        "print(\"Final validation perplexity (if available):\", f\"{history['perplexity'][-1]:.2f}\" if history['perplexity'] and not np.isnan(history['perplexity'][-1]) else \"N/A\")\n",
        "print(\"Qualitative evaluation: Inspect the generated text samples above.\")\n",
        "\n",
        "\n",
        "# --- 8. Plotting with plotnine ---\n",
        "print(\"\\n--- Plotting Training Progress ---\")\n",
        "if history['epoch']: # Check if history has data\n",
        "    history_df = pd.DataFrame(history)\n",
        "\n",
        "    # Plot training and validation loss\n",
        "    loss_plot = (\n",
        "        ggplot(history_df.melt(id_vars=['epoch'], value_vars=['train_loss', 'val_loss'], var_name='metric', value_name='loss'),\n",
        "               aes(x='epoch', y='loss', color='metric')) +\n",
        "        geom_line() +\n",
        "        labs(title=\"Training and Validation Loss\", x=\"Epoch\", y=\"Loss\") +\n",
        "        theme_minimal()\n",
        "    )\n",
        "    print(loss_plot)\n",
        "\n",
        "    # Plot perplexity\n",
        "    if 'perplexity' in history_df.columns and history_df['perplexity'].notna().any():\n",
        "        perplexity_plot = (\n",
        "            ggplot(history_df[history_df['perplexity'].notna()], # Filter out NaN perplexity if val_loader was empty\n",
        "                   aes(x='epoch', y='perplexity')) +\n",
        "            geom_line(color=\"blue\") +\n",
        "            labs(title=\"Validation Perplexity\", x=\"Epoch\", y=\"Perplexity\") +\n",
        "            theme_minimal()\n",
        "        )\n",
        "        print(perplexity_plot)\n",
        "else:\n",
        "    print(\"No training history to plot (e.g., training was skipped).\")\n",
        "\n",
        "print(\"\\nModel training and evaluation complete.\")\n",
        "print(\"To improve: train for more epochs, use more data, tune hyperparameters (embedding_dim, hidden_dim, layers, dropout, learning rate), or try a character-level model for very large vocabularies.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "22b75898c5f940c1ab9de5d22728df9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cda9ddde4523466e9593f1b4d8b0eadf",
              "IPY_MODEL_baaf7bd514354bf0922d83daa25e4bc6",
              "IPY_MODEL_824dc5dfb494465cb982b8ae9875114b"
            ],
            "layout": "IPY_MODEL_add39ed0cf2c43579908e0aecff8e1ce"
          }
        },
        "d3929b4ba07c4b5fbe5f6856f9c7b254": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fe9e7807b29f4b3ba4621a7fe675fcb2",
              "IPY_MODEL_c6400845699f4f63bb44644ee827ced2",
              "IPY_MODEL_d449f64812ca4111a6bf9fa1d03b69f8"
            ],
            "layout": "IPY_MODEL_abe52347b96f4652bd25c567964a62b7"
          }
        },
        "cda9ddde4523466e9593f1b4d8b0eadf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f81dbcc1a92547afb79997efbf49eece",
            "placeholder": "​",
            "style": "IPY_MODEL_bac79d731ce64460aec3d131b630814e",
            "value": "Cleaning text: 100%"
          }
        },
        "baaf7bd514354bf0922d83daa25e4bc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a38a786d330f45fc8dc620a098633e12",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_227eb829838e4aeaa3a750f3cccae923",
            "value": 5
          }
        },
        "824dc5dfb494465cb982b8ae9875114b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e785f78610e8406081a69a3b00705547",
            "placeholder": "​",
            "style": "IPY_MODEL_d429391ea0f74ea49b7bd219760c2d74",
            "value": " 5/5 [00:00&lt;00:00, 186.53it/s]"
          }
        },
        "add39ed0cf2c43579908e0aecff8e1ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe9e7807b29f4b3ba4621a7fe675fcb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f370388c768c4ce1bef1a53f6d06f409",
            "placeholder": "​",
            "style": "IPY_MODEL_a4c109de42a2440db912e944f6f80ced",
            "value": "Generating n-gram sequences: 100%"
          }
        },
        "c6400845699f4f63bb44644ee827ced2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41ea770492cd485ebb91786c0a15d90d",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_91679a6100a243e0b259b05a74259a93",
            "value": 5
          }
        },
        "d449f64812ca4111a6bf9fa1d03b69f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc8846d9b8be4b44a54869252fb8179b",
            "placeholder": "​",
            "style": "IPY_MODEL_270e2244146f469db8158a1780e1c365",
            "value": " 5/5 [00:00&lt;00:00, 268.54it/s]"
          }
        },
        "abe52347b96f4652bd25c567964a62b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f81dbcc1a92547afb79997efbf49eece": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bac79d731ce64460aec3d131b630814e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a38a786d330f45fc8dc620a098633e12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "227eb829838e4aeaa3a750f3cccae923": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e785f78610e8406081a69a3b00705547": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d429391ea0f74ea49b7bd219760c2d74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f370388c768c4ce1bef1a53f6d06f409": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4c109de42a2440db912e944f6f80ced": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41ea770492cd485ebb91786c0a15d90d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91679a6100a243e0b259b05a74259a93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cc8846d9b8be4b44a54869252fb8179b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "270e2244146f469db8158a1780e1c365": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}